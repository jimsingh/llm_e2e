{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f56ec2-ab4d-480b-aba9-ec18f4b76303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa29f49-4a99-4ff3-94b1-14e15393922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import yaml\n",
    "from typing import Any, ClassVar\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    FIELD_SECTIONS: ClassVar[dict[str, list[str]]] = {\n",
    "        \"data\": [\"dataset_path\", \"dataset_name\", \"block_size\"],\n",
    "        \"tokenizer\": [\"encoding_name\", \"eos_token_id\"],\n",
    "        \"model\": [\n",
    "            \"vocab_size\", \"context_length\", \"emb_dim\",\n",
    "            \"n_heads\", \"n_layers\", \"dropout_rate\", \"qkv_bias\", \"mlp_bias\"\n",
    "        ],\n",
    "        \"training\": [\n",
    "            \"num_epochs\", \"learning_rate\", \"weight_decay\", \"beta1\", \"beta2\",\n",
    "            \"grad_clip\", \"warmup_steps\", \"max_steps\", \"batch_size\",\n",
    "            \"eval_iters\", \"eval_interval\", \"save_interval\"\n",
    "        ],\n",
    "        \"system\": [\"device\", \"compile_model\", \"dtype\"],\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Data\n",
    "    dataset_path: str = 'HuggingFaceFW/fineweb-edu'\n",
    "    dataset_name: str = 'sample-10BT'\n",
    "    block_size: int = 1024\n",
    "\n",
    "    # tokenizer\n",
    "    encoding_name: str = 'gpt2'\n",
    "    eos_token_id: int = 50256 # token_id of '<|endoftext|>'\n",
    "    \n",
    "    # Model architecture\n",
    "    vocab_size: int = 50257 # gpt2 BPE n_tokens\n",
    "    context_length: int = 1024\n",
    "    emb_dim: int = 768\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    dropout_rate: float = 0.1\n",
    "    qkv_bias: bool = False\n",
    "    mlp_bias: bool = True \n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_epochs: int = 2\n",
    "    learning_rate: float = 6e-4\n",
    "    weight_decay: float = 0.1\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95\n",
    "    grad_clip: float = 1.0\n",
    "    warmup_steps: int = 2_000\n",
    "    max_steps: int = 600_000\n",
    "    \n",
    "    # Training setup\n",
    "    batch_size: int = 8\n",
    "    log_interval: int = 200\n",
    "    eval_iters: int = 50\n",
    "    eval_interval: int = 1_00\n",
    "    save_interval: int = 10_000\n",
    "    save_filename: str = field(init = False)\n",
    "    \n",
    "    # System\n",
    "    device: str = \"cuda\"\n",
    "    compile_model: bool = True\n",
    "    dtype: str = \"bfloat16\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, yaml_path: str) -> 'GPT2Config':\n",
    "        \"\"\"Load config from nested YAML structure.\"\"\"\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "        \n",
    "        # Flatten nested structure\n",
    "        flat_dict = {}\n",
    "        if config_dict:\n",
    "            for section, params in config_dict.items():\n",
    "                if isinstance(params, dict):\n",
    "                    flat_dict.update(params)\n",
    "                else:\n",
    "                    flat_dict[section] = params\n",
    "            print(f\"loaded config from: {yaml_path}\")\n",
    "        else:\n",
    "            print(f\"no yaml config found in {yaml_path}, using defaults\")\n",
    "        return cls(**flat_dict)\n",
    "    \n",
    "    def to_yaml(self, yaml_path: str) -> None:\n",
    "        \"\"\"Save config to nested YAML structure.\"\"\"\n",
    "        config_dict = {}\n",
    "        for section, fields in self.FIELD_SECTIONS.items():\n",
    "            config_dict[section] = {field: getattr(self, field) for field in fields}\n",
    "        \n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(config_dict, f, default_flow_style=False, indent=2)\n",
    "    \n",
    "    def update_from_dict(self, updates: dict) -> 'GPT2Config':\n",
    "        \"\"\"Create new config with updated values.\"\"\"\n",
    "        config_dict = {k: v for k, v in self.__dict__.items()}\n",
    "        config_dict.update(updates)\n",
    "        return self.__class__(**config_dict)\n",
    "    \n",
    "    @property\n",
    "    def n_params(self) -> int:\n",
    "        \"\"\"Estimate model parameters in millions.\"\"\"\n",
    "        # Token embedding: vocab_size * emb_dim\n",
    "        token_emb = self.vocab_size * self.emb_dim\n",
    "        \n",
    "        # Position embedding: context_length * emb_dim\n",
    "        pos_emb = self.context_length * self.emb_dim\n",
    "        \n",
    "        # Transformer blocks\n",
    "        # Each block: 4 * emb_dim^2 (attention) + 8 * emb_dim^2 (mlp) = 12 * emb_dim^2\n",
    "        transformer_blocks = self.n_layers * 12 * self.emb_dim**2\n",
    "\n",
    "        # 2 LayerNorms per block (before attention, before MLP) + 1 final LayerNorm after transformer blocks.\n",
    "        num_layernorms = (2 * self.n_layers) + 1\n",
    "        \n",
    "        # Each LayerNorm has 2 parameters (scale gamma, bias beta) per embedding dimension.\n",
    "        layer_norms = num_layernorms * (2 * self.emb_dim)\n",
    "        \n",
    "        total_params = token_emb + pos_emb + transformer_blocks + layer_norms\n",
    "        return total_params\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        assert self.emb_dim % self.n_heads == 0, f\"emb_dim ({self.emb_dim}) must be divisible by n_heads ({self.n_heads})\"\n",
    "        assert self.context_length == self.block_size, f\"context_length ({self.context_length}) should equal block_size ({self.block_size})\"\n",
    "        self.save_filename = f\"{self.dataset_path.replace(\"/\",\"_\")}.{self.n_params}.pt\"\n",
    "\n",
    "#if False:\n",
    "    # Load from YAML\n",
    "    #config = GPT2Config.from_yaml(\"gpt2_config.yaml\")\n",
    "    #print(f\"Loaded config: {config.emb_dim} dim, {config.n_heads} heads\")\n",
    "    #print(f\"Estimated parameters: {config.n_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
