{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2995a6-ffb2-4614-812d-a49db6553f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OpenAI's GPT2 paper, github repo, and pre-trained weights give us a good idea of the model architecture, \n",
    "but there isn't enough information available to actually train a model. \n",
    "\n",
    "Andrej Karpathy provides a working implementation, but even so, the intution isn't expressed in that code. Also\n",
    "missing are the (probably hard won) insights required to go from theory to working code.\n",
    "\n",
    "I'll try to build GPT2 including those key insights and document it here. \n",
    "\n",
    "references:\n",
    "* paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "* repo: https://github.com/openai/gpt-2\n",
    "* kaparthy's nanoGPT: https://github.com/karpathy/nanoGPT/tree/master\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038b608-1a32-48a8-bde1-a77373baf7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The high-level tensorflow model structure released by OpenAI translated to pytorch.\n",
    "    \n",
    "    src: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L147\n",
    "    \n",
    "    def model(hparams, X, past=None, scope='model', reuse=False):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "                \n",
    "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # positional embeddings - each position in the input gets a learned positional embedding to capture relationships\n",
    "        # between words. Worth noting, AIAYN used fixed embeddings while GPT and BERT uses learned embeddings.\n",
    "        #\n",
    "        # wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd], initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        self.position_embeddings = nn.Embedding(num_embeddings=cfg.context_length, embedding_dim=cfg.emb_dim)\n",
    "        torch.nn.init.normal_(self.position_embeddings.weight, mean=0.0, std=0.01)\n",
    "\n",
    "        # token embeddings - map token ids to learned token embeddings\n",
    "        #\n",
    "        # wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd], initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=cfg.vocab_size, embedding_dim=cfg.emb_dim)\n",
    "        torch.nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(cfg.dropout_rate)\n",
    "        \n",
    "        # positional and token embeddings are added together to represent both the word and where it\n",
    "        # is in the input. this is the input to the transformer blocks.\n",
    "\n",
    "        # transformer model is a stack of transformer blocks. the hparam cfg.n_layers tells us how many\n",
    "        #\n",
    "        #    presents = []\n",
    "        #    for layer, past in enumerate(pasts):\n",
    "        #        h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "        #        presents.append(present)\n",
    "        #    results['present'] = tf.stack(presents, axis=1)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "        # normalize (stabilize) to unit variance and learn gain and bias\n",
    "        # \n",
    "        #    norm from h = norm(h, 'ln_f')\n",
    "        self.final_norm = LayerNorm(cfg.emb_dim)\n",
    "    \n",
    "        # project hidden states (h) to logits tying weights to token_embeddings (so we end up with 1 set of\n",
    "        # embeddings for tokens used in both input and output.\n",
    "        # Corresponds to TF: `logits = tf.matmul(h_flat, wte, transpose_b=True)`\n",
    "\n",
    "        # F.linear(input, weight) applies a linear transformation to the last dimension of the hidden layer\n",
    "        # h: (B, T, C), self.token_embedding.weight: (vocab_size, C) -> (B, T, vocab_size)\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False)\n",
    "        \n",
    "        # Tie the weights of the output projection layer with the token embedding layer\n",
    "        self.out_head.weight = self.token_embedding.weight\n",
    "\n",
    "        # tiktoken gpt2 encoding, this is the token id of '<|endoftext|>'\n",
    "        self.eos_token_id = cfg.eos_token_id\n",
    "\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor, targets: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_len = in_idx.shape # token indices (B, T)\n",
    "\n",
    "        # get the token embeddings for each token index (B, T) -> (B, T, C)\n",
    "        tok_embeds = self.token_embedding(in_idx)\n",
    "\n",
    "        # get position embeddings for each sequence index (T)\n",
    "        pos_indices = torch.arange(seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.position_embeddings(pos_indices) # (T) -> (T, C)\n",
    "\n",
    "        # Combines token and position embeddings for input to transformer blocks\n",
    "        # \n",
    "        #    h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "        x = tok_embeds + pos_embeds # (B, T, C) + (T, C) -> (B, T, C) - broadcasting works right to left\n",
    "        s = self.emb_dropout(x)\n",
    "        \n",
    "        # replaces the for layer, past in enumerate(pasts) loop\n",
    "        # n.Sequential replaces the `for layer, past in enumerate(pasts)` loop\n",
    "        h = self.transformer_blocks(x)\n",
    "\n",
    "        # h = norm(h, 'ln_f')\n",
    "        h = self.final_norm(h)\n",
    "        \n",
    "        # hidden state to logits, linear C -> cfg.vocab_size\n",
    "        logits = self.out_head(h) # (B, T, C) -> (B, T, cfg.vocab_size)\n",
    "\n",
    "        # compute the loss if given targets\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten(), ignore_index=-1)\n",
    "        \n",
    "        # todo: optimize logits calculation if we're only performing inference\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_tokens, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        \n",
    "        context_length = self.position_embeddings.weight.shape[0]\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            # crop to the context length\n",
    "            idx_cond = idx if idx.size(1) <= context_length else idx[:, -context_length:]\n",
    "            \n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            # TODO: KV caching so that we don't recompute every \n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # scale by temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # check for end of sequence token and stop generation.\n",
    "            if idx_next.item() == self.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        self.train()\n",
    "        return idx\n",
    "    \n",
    "            \n",
    "    def save_parameters(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Saves model parameters (state_dict) to a specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path where the state_dict will be saved.\n",
    "        \"\"\"\n",
    "        current_state_dict = self.state_dict()\n",
    "        \n",
    "        # Save the state dictionary to the specified file\n",
    "        torch.save(current_state_dict, file_path)\n",
    "        \n",
    "        print(f\"Model parameters saved to {file_path}\")\n",
    "        \n",
    "    def load_parameters(self, file_path: str, mod_prefix='_orig_mod.', map_location=None):\n",
    "        \"\"\"\n",
    "        Loads model parameters (state_dict) from a specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the file containing the state_dict.\n",
    "            remove_prefix (str): prefix to remove (often created by model.compile)\n",
    "        \"\"\"\n",
    "\n",
    "        state_dict = torch.load(file_path, map_location=map_location)\n",
    "        has_prefix = any(k.startswith(mod_prefix) for k in state_dict.keys())\n",
    "\n",
    "        if has_prefix:\n",
    "            state_dict = {\n",
    "                k.removeprefix(mod_prefix): v\n",
    "                for k, v in state_dict.items()\n",
    "            }\n",
    "        self.load_state_dict(state_dict)\n",
    "        \n",
    "        print(f\"Model parameters loaded from {file_path}\")\n",
    "\n",
    "            \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1), persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        \n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # the mask fills future token positions with -inf\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim**0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # project back to context attention @ values \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.transpose(1, 2)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # output projection\n",
    "\n",
    "        return context_vec, attn_weights\n",
    "        \n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" \n",
    "    Implementation of Transformer Blocks without KV caching (not critical for training)\n",
    "\n",
    "    src: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L123C1-L130C26\n",
    "    \n",
    "    def block(x, scope, *, past, hparams):\n",
    "        with tf.variable_scope(scope):\n",
    "            nx = x.shape[-1].value\n",
    "            a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "            x = x + a\n",
    "            m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "            x = x + m\n",
    "            return x, present\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg.emb_dim, d_out=cfg.emb_dim,\n",
    "            context_length=cfg.context_length,\n",
    "            num_heads=cfg.n_heads,\n",
    "            dropout=cfg.dropout_rate,\n",
    "            qkv_bias=cfg.qkv_bias)\n",
    "        \n",
    "        self.ff = FeedForward(cfg) # ff = mlp\n",
    "        self.norm1 = LayerNorm(cfg.emb_dim) # norm(x, 'ln_1')\n",
    "        self.norm2 = LayerNorm(cfg.emb_dim) # norm(x, 'ln_2')\n",
    "        self.dropout = nn.Dropout(cfg.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implements the transformer pre-norm forward pass:\n",
    "    \n",
    "            X_in -> Norm_1 -> Attention (A)     -> Shortcut (X_in + A)   -> X_a\n",
    "            X_a  -> Norm_2 -> FeedForward (MLP) -> Shortcut (X_a + MLP) -> X_out\n",
    "\n",
    "        \"\"\"\n",
    "        # x_in -> Norm1 -> Attn -> Residual -> x_a\n",
    "        x_n = self.norm1(x) \n",
    "        a, _ = self.att(x_n) # ignore attn weights\n",
    "        a = self.dropout(a)\n",
    "        x_a = x + a            # (B, T, C) + (B, T, C) (add residual) -> (B, T, C)\n",
    "        \n",
    "        # feedforward\n",
    "        x_na = self.norm2(x_a)\n",
    "        x_mlp = self.ff(x_na)\n",
    "        x_mlp = self.dropout(x_mlp)\n",
    "        x_out = x_a + x_mlp # add residual (shortcut)\n",
    "        return x_out\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements layer normalization\n",
    "    \n",
    "    src: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L28\n",
    "    \n",
    "    def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "        # normalize to mean = 0, std = 1, then do a diagonal affine transform.\n",
    "        with tf.variable_scope(scope):\n",
    "            n_state = x.shape[-1].value\n",
    "            g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
    "            b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
    "            u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "            s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "            x = (x - u) * tf.rsqrt(s + epsilon)\n",
    "            x = x*g + b\n",
    "            return x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.gain = nn.Parameter(torch.ones(dim)) # scale\n",
    "        self.bias = nn.Parameter(torch.zeros(dim)) # shift\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # n is large enough that biased vs unbiased shouldn't matter, but this is what GPT2 does\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # from the gpt2 source: \"then do a diagonal affine transform.\"\n",
    "        #\n",
    "        # english translation:\n",
    "        #     affine transfrom: y = Wx + b, \"diagonal\" means W is diagonal (other elements are 0)\n",
    "        #     this effectively means that the transfrom is *per feature* and the gain + shift\n",
    "        #     is learned for each feature without any cross-feature interactions.\n",
    "        return self.gain * norm_x + self.bias\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed Forward from AIAYN. The module expands input features, applies activation,\n",
    "    and then projects them back to the original dimensions. The idea is to learn more expressive\n",
    "    relationships between features.\n",
    "    \n",
    "    note: GPT2 replaces ReLU with GELU.\n",
    "\n",
    "    from: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L115\n",
    "    \n",
    "    def mlp(x, scope, n_state, *, hparams):\n",
    "        with tf.variable_scope(scope):\n",
    "            nx = x.shape[-1].value\n",
    "            h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "            h2 = conv1d(h, 'c_proj', nx)\n",
    "            return h2\n",
    "\n",
    "    def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "        with tf.variable_scope(scope):\n",
    "            *start, nx = shape_list(x) # nx is the input feature dimension (in_channels)\n",
    "            w = tf.get_variable('w', [1, nx, nf], # Kernel shape: [filter_width, in_channels, out_channels]\n",
    "                                initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
    "            b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
    "            c = tf.nn.conv1d(x, w, stride=1, padding='VALID') + b # Uses tf.nn.conv1d\n",
    "            return c\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        expansion_factor = 4\n",
    "\n",
    "        # this was ... confusing. AIAYN uses a conv1d with a kernel size of 1 - meaning the convolution\n",
    "        # operates on a single feature at a time. This is the same thing as nn.Linear which operates on the last\n",
    "        # dimension of the input (in this case the embedding features).\n",
    "        self.expansion = nn.Linear(cfg.emb_dim, expansion_factor * cfg.emb_dim) # conv1d expands (B, T, C) -> (B, T, 4C) \n",
    "        self.gelu = nn.GELU() \n",
    "        self.projection = nn.Linear(expansion_factor * cfg.emb_dim, cfg.emb_dim, bias=cfg.mlp_bias) # conv1d projects back to (B, T, C)\n",
    "        self.dropout = nn.Dropout(cfg.dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Asserts to make sure I'm thinking about this correctly\n",
    "        # x initial shape: (B, T, C) where C is cfg.emb_dim\n",
    "        assert x.ndim == 3, f\"Input tensor expected to be 3D (B, T, C), got {x.ndim}.\"\n",
    "        assert x.shape[-1] == self.expansion.in_features, \\\n",
    "            f\"Input feature dimension mismatch. Expected {self.expansion.in_features}, but got {x.shape[-1]}.\"\n",
    "\n",
    "        # The input and output dimensions should match, store them for the final assertion\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        x = self.expansion(x)   # (B, T, C) -> (B, T, 4*C)\n",
    "        x = self.gelu(x)        # (B, T, 4*C) -> (B, T, 4*C)\n",
    "        x = self.dropout(x)\n",
    "        x = self.projection(x)  # (B, T, 4*C) -> (B, T, C)\n",
    "\n",
    "        assert x.shape == (B, T, C), f\"Output shape mismatch. Expected {(B, T, C)}, but got {x.shape}.\"\n",
    "        return x\n",
    "\n",
    "# GPT2 replace ReLU with Gaussian Error Linear Unit (GELU) as a smoother activation function.\n",
    "# GELU based on math.erf (CDF of the standard normal distribution). For GELU, the integration\n",
    "# of e^{-t^2} is approximated by a manually fit function of tanh. I believe GELU was used by BERT.\n",
    "#\n",
    "#    gelu(x):\n",
    "#      return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "#\n",
    "# paper: https://arxiv.org/pdf/1606.08415\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
