{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2995a6-ffb2-4614-812d-a49db6553f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.06ms\u001b[0m\u001b[0m\n",
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=5, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... estimated batches: 58\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "Total tokens analyzed: 102,400\n",
      "Unique tokens: 7013\n",
      "Top 10 tokens:\n",
      "  ID 198   ('\\n'      ): 12,382 (0.1209)\n",
      "  ID 11    (','       ): 5,909  (0.0577)\n",
      "  ID 25    (':'       ): 3,139  (0.0307)\n",
      "  ID 13    ('.'       ): 2,362  (0.0231)\n",
      "  ID 262   (' the'    ): 1,753  (0.0171)\n",
      "  ID 284   (' to'     ): 1,298  (0.0127)\n",
      "  ID 286   (' of'     ): 1,090  (0.0106)\n",
      "  ID 290   (' and'    ): 1,083  (0.0106)\n",
      "  ID 26    (';'       ): 1,003  (0.0098)\n",
      "  ID 314   (' I'      ): 997    (0.0097)\n",
      "Vocabulary coverage: 0.13954275026364488\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "x: [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198]\n",
      "y: [22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198]\n",
      "x: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n'\n",
      "y: ' Citizen:\\nBefore we proceed any further, hear me speak.\\n\\n'\n",
      "x: [514, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546]\n",
      "y: [13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546, 408]\n",
      "x: ' us.\\n\\nMENENIUS:\\nEither you must\\nConf'\n",
      "y: '.\\n\\nMENENIUS:\\nEither you must\\nConfess'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OpenAI's GPT2 paper, github repo, and pre-trained weights give us a good idea of the model architecture, \n",
    "but there isn't enough information available to actually train a model. \n",
    "\n",
    "Andrej Karpathy provides a working implementation, but even so, the intution isn't expressed in that code. Also\n",
    "missing are the (probably hard won) insights required to go from theory to working code.\n",
    "\n",
    "I'll try to build GPT2 including those key insights and document it here. \n",
    "\n",
    "references:\n",
    "* paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "* repo: https://github.com/openai/gpt-2\n",
    "* kaparthy's nanoGPT: https://github.com/karpathy/nanoGPT/tree/master\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ec221-8448-426e-a020-13d8e8fd6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6038b608-1a32-48a8-bde1-a77373baf7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The high-level tensorflow model structure released by OpenAI translated to pytorch. Comments are my own\n",
    "    \n",
    "    from: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L147\n",
    "    \n",
    "    def model(hparams, X, past=None, scope='model', reuse=False):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "                \n",
    "        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # positional embeddings - each position in the input gets a learned positional embedding to capture relationships\n",
    "        # between words. Worth noting, AIAYN used fixed embeddings while GPT and BERT uses learned embeddings.\n",
    "        #\n",
    "        # wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd], initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        self.position_embeddings = nn.Embedding(num_embeddings=cfg['context_length'], embedding_dim=cfg['emb_dim'])\n",
    "        torch.nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.01)\n",
    "\n",
    "        # token embeddings - weights learned by mapping token ids to these embedings.\n",
    "        #\n",
    "        # wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd], initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=cfg['vocab_size'], embedding_dim=cfg['emb_dim'])\n",
    "        torch.nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        # positional and token embeddings are added together to represent both the word and where it\n",
    "        # is in the input. this is the input to the transformer blocks.\n",
    "\n",
    "        # transformer model is a stack of transformer blocks. the hparam cfg['transformer_layers'] tells us how many to use\n",
    "        # this is a key input into the model's overall size.\n",
    "        #\n",
    "        #    presents = []\n",
    "        #    for layer, past in enumerate(pasts):\n",
    "        #        h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "        #        presents.append(present)\n",
    "        #    results['present'] = tf.stack(presents, axis=1)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['transformer_layers'])])\n",
    "\n",
    "        # stabalize output: normalize to unit variance and train: gain and bias\n",
    "        # \n",
    "        #    define: norm(h, 'ln_f')\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "    \n",
    "        # projection head: transform final hiden layer into logits for every vocab token (i.e. predict next token)\n",
    "        # self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = in_idx.shape # token indices (B, T)\n",
    "\n",
    "        # get the token embeddings for each token index (B, T) -> (B, T, C)\n",
    "        tok_embeds = self.token_embedding(in_idx)\n",
    "\n",
    "        # get position embeddings for each sequence index (T)\n",
    "        pos_indices = torch.arange(seq_len, device=in_idx.device, dtype=torch.long)\n",
    "        pos_embeds = self.position_embeddings(pos_indices) # (T, C)\n",
    "\n",
    "        # Combine token and position embeddings for input to transformer blocks\n",
    "        # \n",
    "        #    h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "        x = tok_embeds + pos_embeds # (B, T, C) + (B, T, C) -> (B, T, C)\n",
    "\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        # stability and output projection\n",
    "        #\n",
    "        #    apply: h = norm(h, 'ln_f')\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        return x\n",
    "        # return self.out_head(x) # logits\n",
    "        # note: parameter sharing withe input embedding\n",
    "        # logits = F.linear(x, self.token_embedding.weight)\n",
    "        #return logits\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" \n",
    "    Implementation of Transfomer Blocks\n",
    "    \n",
    "    note: I'm leaving out past_kv for now. It is an important optimization to avoid recomputing KV for each token\n",
    "    in the sequence, but it adds complexity and I need to figure out how to implement it correctly. For training,\n",
    "    kv caching isn't essential.\n",
    "    \n",
    "    from: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L123C1-L130C26\n",
    "    \n",
    "    def block(x, scope, *, past, hparams):\n",
    "        with tf.variable_scope(scope):\n",
    "            nx = x.shape[-1].value\n",
    "            a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "            x = x + a\n",
    "            m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "            x = x + m\n",
    "            return x, present\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'], d_out=cfg['emb_dim'],\n",
    "            context_length=cfg['context_length'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            dropout=cfg['dropout_rate'],\n",
    "            qkv_bias=cfg['qkv_bias'])\n",
    "        \n",
    "        self.ff = FeedForward(cfg) # ff = mlp\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim']) # norm(x, 'ln_1')\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim']) # norm(x, 'ln_2')\n",
    "\n",
    "    def forward(self, x): \n",
    "        nx = x # shortcut keeps the original input to help prevent vanishing gradients\n",
    "        nx = self.norm1(x)\n",
    "        a = self.att(nx)\n",
    "        x = x + a\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of norm (https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L28)\n",
    "    \n",
    "    def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "        # normalize to mean = 0, std = 1, then do a diagonal affine transform.\n",
    "        with tf.variable_scope(scope):\n",
    "            n_state = x.shape[-1].value\n",
    "            g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n",
    "            b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n",
    "            u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "            s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "            x = (x - u) * tf.rsqrt(s + epsilon)\n",
    "            x = x*g + b\n",
    "            return x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.gain = nn.Parameter(torch.ones(dim)) # scale\n",
    "        self.bias = nn.Parameter(torch.zeros(dim)) # shift\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # n is large enough that biased vs unbiased shouldn't matter, but this is what GPT2 does\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # \"then do a diagonal affine transform.\"\n",
    "        # translation:\n",
    "        # affine transfrom: y = Wx + b, diagonal means the transfrom is per feature\n",
    "        # and the features don't interact. In essense, we learn a scale and shift for\n",
    "        # each feature so that signals are appropriately strong after normalization \n",
    "        return self.gain * norm_x + self.bias\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed Forward from AIAYN. The module expands input features, applies activation,\n",
    "    and then projects them back to the original dimensions. The idea is to learn more expressive\n",
    "    relationships between features.\n",
    "    \n",
    "    note: GPT2 replaces ReLU with GELU.\n",
    "\n",
    "    from: https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L115\n",
    "    \n",
    "    def mlp(x, scope, n_state, *, hparams):\n",
    "        with tf.variable_scope(scope):\n",
    "            nx = x.shape[-1].value\n",
    "            h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "            h2 = conv1d(h, 'c_proj', nx)\n",
    "            return h2\n",
    "\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg['emb_dim']\n",
    "        expansion = 4 # from attention is all you need\n",
    "        self.expansion = nn.Linear(emb_dim, self.expansion * emb_dim) # conv1d: expand x \n",
    "        self.gelu = nn.GELU() \n",
    "        self.projection = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # conv1d \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.expansion(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# GPT2 replace ReLU with Gaussian Error Linear Unit (GELU) as a smoother activation function.\n",
    "# GELU based on math.erf (CDF of the standard normal distribution). For GELU, the integration\n",
    "# of e^{-t^2} is approximated by a manually fit function of tanh. I believe GELU was used by BERT.\n",
    "#\n",
    "#    gelu(x):\n",
    "#      return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "#\n",
    "# paper: https://arxiv.org/pdf/1606.08415\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
