{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ba3472-3785-449e-a4ea-c5ad1b719b67",
   "metadata": {},
   "source": [
    "### Notebook focuses on loading and preparing datasets from Hugging Face\n",
    "\n",
    "Goal is to write dataset loaders tha can\n",
    "* support streaming and shuffling\n",
    "* creates test and validation splits\n",
    "* are debuggable\n",
    "* handle tokenization, encoding / decoding\n",
    "* creates x,y pairs for training (shift by one and potentially other strategies)\n",
    "\n",
    "---\n",
    "### TODO\n",
    "* Prepare datasets for **finetuning** (IT and Classification)\n",
    "* Masking collation (BERT style)\n",
    "* FIM collation (used for code completion)\n",
    "  \n",
    "### References:\n",
    "* [tiktoken](https://github.com/openai/tiktoken) - I'll likely stick to gpt2 BPE encoding for size reasons\n",
    "* [huggingface datasets](https://huggingface.co/docs/datasets/en/stream) - shows how to stream huggingface datasets\n",
    "* datasets\n",
    "  - karpathy/tiny_shakespeare (the dataset used in Andrej Karpathy's blog post about the unreasonable effectiveness of RNNs)\n",
    "  - HuggingFaceFW/fineweb-edu (10B tokens, curated to be high quality)\n",
    "  - imdb or rotten tomatoes (for classification)\n",
    "  - TBD for instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc9fcd5-531e-4c67-b42e-23cb2d99fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 0.47ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add datasets tiktoken\n",
    "import logging\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "import datasets as hf_datasets\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "log: logging.Logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f6cf64b-1bc1-41f7-ac98-bb6b9ab1e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as hf_datasets\n",
    "\n",
    "class ShakespeareDataloader:\n",
    "    DATASET_PATH = 'karpathy/tiny_shakespeare'\n",
    "    \n",
    "    def __init__(self, batch_size, sequence_length, tokenizer, text_col=\"text\", split=\"train\", shuffle=True):\n",
    "        \"\"\"Causal Dataloader for the 'karpathy/tiny_shakespeare' dataset, loaded fully into memory.\"\"\"\n",
    "        self._name = f\"{self.__class__.__name__}\"\n",
    "        self.B = batch_size\n",
    "        self.T = sequence_length\n",
    "        self.split = split        \n",
    "        self.tokens_per_step = self.B * self.T\n",
    "    \n",
    "        assert self.tokens_per_step > 0, f\"invalid number of tokens per step {self.B} * {self.T} = {self.tokens_per_step}\"\n",
    "\n",
    "        print(f\"{self._name} Initializing: {ShakespeareDataset.DATASET_PATH} with B={self.B}, T={self.T}, split='{self.split}'\")\n",
    "        dataset = hf_datasets.load_dataset(ShakespeareDataset.DATASET_PATH, name=\"default\", split=self.split, streaming=False)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "        print(f\"{self._name} Pre-tokenizing text data n={len(dataset['text'][0]):,} for split '{self.split}'... \", end='')\n",
    "        \n",
    "        tokenized_dataset = dataset.map(tokenizer, batched=True)\n",
    "        \n",
    "        flat_tokens = [token for tokens in tokenized_dataset['tokens'] for token in tokens]\n",
    "        \n",
    "        self.all_tokens = torch.tensor(flat_tokens, dtype=torch.long)\n",
    "        self.total_tokens = len(self.all_tokens)\n",
    "        self.yieldable_batches = (self.total_tokens - 1) // (self.tokens_per_step) # note the -1 because we need 1 more token to create y\n",
    "        \n",
    "        print(f\"estimated batches: {self.yieldable_batches}\")\n",
    "        assert self.yieldable_batches > 0, f\"not enough tokens to yield a full batch. {self.total_tokens}, {self.tokens_per_step}\"\n",
    "\n",
    "        # for iter\n",
    "        self.current_idx = 0\n",
    "        self.batches_yielded = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"each new epoch will move the iter to the start of the dataset.\"\"\"\n",
    "        self.current_idx = 0\n",
    "        self.batches_yielded = 0\n",
    "        print(f\"{self._name} iterator reset for split '{self.split}', starting at token 0\")\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Yield the next batch of token indices where y is shifted by 1 from x.\n",
    "\n",
    "        Returns:\n",
    "            a tuple (x, y) where both tensors have shape (B, T) \n",
    "        \"\"\"\n",
    "        start_idx = self.current_idx\n",
    "        \n",
    "        if start_idx + self.tokens_per_step + 1 > self.total_tokens:\n",
    "            print(f\"{self._name} No more tokens: __next__: Not enough tokens for a full batch from index {start_idx}. \"\n",
    "                  f\"Needed {self.tokens_per_step}, available: {self.total_tokens - start_idx} tokens.\")\n",
    "            raise StopIteration\n",
    "\n",
    "        batch_tokens = self.all_tokens[start_idx : start_idx + self.tokens_per_step + 1]\n",
    "        \n",
    "        x = batch_tokens[:-1].view(self.B, self.T)\n",
    "        y = batch_tokens[1:].view(self.B, self.T)\n",
    "        \n",
    "        self.current_idx += self.tokens_per_step\n",
    "        self.batches_yielded += 1\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the estimated number of batches per epoch.\"\"\"\n",
    "        return self.yieldable_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f927b573-f9d8-41c4-be63-0c0591a71e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=5, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1003854 for split 'train'... estimated batches: 58\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "Total tokens analyzed: 102,400\n",
      "Unique tokens: 7013\n",
      "Top 10 tokens:\n",
      "  ID 198   ('\\n'      ): 12,382 (0.1209)\n",
      "  ID 11    (','       ): 5,909  (0.0577)\n",
      "  ID 25    (':'       ): 3,139  (0.0307)\n",
      "  ID 13    ('.'       ): 2,362  (0.0231)\n",
      "  ID 262   (' the'    ): 1,753  (0.0171)\n",
      "  ID 284   (' to'     ): 1,298  (0.0127)\n",
      "  ID 286   (' of'     ): 1,090  (0.0106)\n",
      "  ID 290   (' and'    ): 1,083  (0.0106)\n",
      "  ID 26    (';'       ): 1,003  (0.0098)\n",
      "  ID 314   (' I'      ): 997    (0.0097)\n",
      "Vocabulary coverage: 0.13954275026364488\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "x: [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198]\n",
      "y: [22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198]\n",
      "x: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n'\n",
      "y: ' Citizen:\\nBefore we proceed any further, hear me speak.\\n\\n'\n",
      "x: [514, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546]\n",
      "y: [13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546, 408]\n",
      "x: ' us.\\n\\nMENENIUS:\\nEither you must\\nConf'\n",
      "y: '.\\n\\nMENENIUS:\\nEither you must\\nConfess'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def print_xy(loader, tokenizer, n=2, tokens=15):\n",
    "    x, y = next(iter(loader))\n",
    "    for i in range(min(n, x.shape[0])):\n",
    "        xs, ys = x[i][:tokens], y[i][:tokens]\n",
    "        print(f\"x: {xs.tolist()}\")\n",
    "        print(f\"y: {ys.tolist()}\")\n",
    "        print(f\"x: {repr(tokenizer.decode(xs.tolist()))}\")\n",
    "        print(f\"y: {repr(tokenizer.decode(ys.tolist()))}\")\n",
    "        \n",
    "def check_data_quality(loader, tokenizer, num_batches=5):\n",
    "    \"\"\"Simple data quality tests to ensure basic functionality is working\"\"\"\n",
    "    total_tokens = 0\n",
    "    freq_counts = Counter()\n",
    "\n",
    "    for (input_batch, target_batch) in itertools.islice(loader, num_batches):\n",
    "        freq_counts.update(input_batch.flatten().tolist())\n",
    "        total_tokens += input_batch.numel()\n",
    "\n",
    "    print(f\"Total tokens analyzed: {total_tokens:,}\")\n",
    "    print(f\"Unique tokens: {len(freq_counts)}\")\n",
    "    print(f\"Top 10 tokens:\")\n",
    " \n",
    "    for token_id, count in freq_counts.most_common(10):\n",
    "        token_text = repr(tokenizer.decode([token_id]))\n",
    "        frac = (count / total_tokens)\n",
    "        print(f\"  ID {token_id:<5d} ({token_text:<10}): {count:<6,d} ({frac:.4f})\")\n",
    "\n",
    "    # check for coverage issues (generally too little)\n",
    "    vocab_coverage = len(freq_counts) / tokenizer.n_vocab\n",
    "    print(f\"Vocabulary coverage: {vocab_coverage}\")\n",
    "\n",
    "    # print x, y pairs to verify collation\n",
    "    print_xy(loader, tokenizer)\n",
    "    return freq_counts\n",
    "\n",
    "gpt2 = tiktoken.get_encoding('gpt2')\n",
    "tokenizer = lambda r: {'tokens': gpt2.encode_batch(r['text'])}\n",
    "check_data_quality(ShakespeareDataloader(5, 1024, tokenizer), gpt2, 20)\n",
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
