{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d34584-21a3-493c-a8ef-21bab935902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook looks at attention (KQ) weights to visualize how model heads\n",
    "# are connecting (attending) different tokens in the context window.\n",
    "# we look at the final layer because it's the most important for \n",
    "# next token generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41623dde-b163-4c71-ab1e-944a0c0ec4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from llm_e2e.model import GPT2Model, MultiHeadAttention\n",
    "from llm_e2e.config import GPT2Config\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    def __init__(self, model: GPT2Model, encoding: tiktoken.Encoding):\n",
    "        self.model = model\n",
    "        self.encoding = encoding\n",
    "        self._hooks = []\n",
    "        self._attention_weights = []\n",
    "\n",
    "    def _hook_fn(self, module: torch.nn.Module, inputs: tuple, output: tuple):\n",
    "        # output of MultiHeadAttention is (context_vec, attn_weights)\n",
    "        self._attention_weights.append(output[1].detach())\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MultiHeadAttention):\n",
    "                hook = module.register_forward_hook(self._hook_fn)\n",
    "                self._hooks.append(hook)\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        for hook in self._hooks:\n",
    "            hook.remove()\n",
    "        self._hooks = []\n",
    "\n",
    "    @contextmanager\n",
    "    def capture_attention(self):\n",
    "        \"\"\"a context manager to handle hook registration and removal.\"\"\"\n",
    "        self._register_hooks()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            self._remove_hooks()\n",
    "\n",
    "    def get_attention_for_prompt(self, prompt: str) -> list[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        gets attention weights for the prompt string. must be called\n",
    "        within the `capture_attention` context.\n",
    "        \"\"\"\n",
    "        if not self._hooks:\n",
    "            print(\"no hooks registered\")\n",
    "            return None\n",
    "\n",
    "        self._attention_weights = []\n",
    "        self.model.eval()\n",
    "\n",
    "        encoded_ids = self.encoding.encode(prompt)\n",
    "        input_tensor = torch.tensor([encoded_ids], dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.model(input_tensor)\n",
    "\n",
    "        if not self._attention_weights:\n",
    "            print(\"no attention weights captured.\")\n",
    "            return None\n",
    "\n",
    "        return self._attention_weights\n",
    "\n",
    "    def plot_attention_heads(\n",
    "        self,\n",
    "        attention_weights: list[torch.Tensor],\n",
    "        prompt: str,\n",
    "        layer_idx: int = -1,\n",
    "        heads_to_show: int = 8,\n",
    "        grid_dims: tuple[int, int] = (2, 4),\n",
    "        save_fig: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the attention heatmaps for specified heads in a given layer.\n",
    "        \"\"\"\n",
    "        if layer_idx < 0:\n",
    "            layer_idx += len(attention_weights)\n",
    "\n",
    "        if not 0 <= layer_idx < len(attention_weights):\n",
    "            print(f\"error: invalid layer index {layer_idx}. model has {len(attention_weights)} layers.\")\n",
    "            return\n",
    "\n",
    "        attention_matrix = attention_weights[layer_idx]\n",
    "        tokens = [self.encoding.decode([i]) for i in self.encoding.encode(prompt)]\n",
    "\n",
    "        num_heads_available = attention_matrix.shape[1]\n",
    "        heads_to_plot = min(heads_to_show, num_heads_available)\n",
    "\n",
    "        nrows, ncols = grid_dims\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 5.5 * nrows), squeeze=False)\n",
    "        fig.suptitle(f\"Attention Patterns - Layer {layer_idx + 1}\", fontsize=16)\n",
    "\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            attn_head = attention_matrix[0, i, :, :].cpu().float().numpy()\n",
    "\n",
    "            sns.heatmap(\n",
    "                attn_head,\n",
    "                ax=ax,\n",
    "                xticklabels=tokens,\n",
    "                yticklabels=tokens,\n",
    "                annot=False,\n",
    "                cmap='Blues',\n",
    "                cbar=True\n",
    "            )\n",
    "\n",
    "            ax.set_title(f\"Head {i + 1}\", fontsize=12)\n",
    "            ax.set_xlabel(\"Key Tokens\", fontsize=10)\n",
    "            ax.tick_params(axis='x', rotation=75, labelsize=9)\n",
    "            ax.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(\"Query Tokens\", fontsize=10)\n",
    "            else:\n",
    "                ax.tick_params(axis='y', labelleft=True)\n",
    "        \n",
    "        fig.tight_layout(pad=1.5)\n",
    "        plt.show()\n",
    "        \n",
    "        if save_fig:\n",
    "            plt.savefig(save_fig + f\"_{layer_idx}.png\", bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f808248-b7cc-4924-9b3a-973f1d063a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from llm_e2e.config import GPT2Config\n",
    "from llm_e2e.model import GPT2Model\n",
    "\n",
    "cfg = GPT2Config.from_yaml('../config/gpt2_bert_corpus_gpu.yaml')\n",
    "prompt = \"The fast brown airplane flew over the lazy dog. It then landed at the airport.\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(cfg.encoding_name)\n",
    "\n",
    "gpt_model = GPT2Model(cfg)\n",
    "d = torch.load('../models/gpt_bert.pth', weights_only=True, map_location=torch.device('cpu'))\n",
    "gpt_model.load_state_dict(d)\n",
    "\n",
    "visualizer = AttentionVisualizer(gpt_model, encoding)\n",
    "\n",
    "attention_weights = None\n",
    "\n",
    "with visualizer.capture_attention():\n",
    "    attention_weights = visualizer.get_attention_for_prompt(prompt)\n",
    "    visualizer.plot_attention_heads(\n",
    "            attention_weights=attention_weights,\n",
    "            prompt=prompt,\n",
    "            layer_idx=-1,\n",
    "            heads_to_show=cfg.n_heads,\n",
    "            save_fig=\"attention_gpt_bert\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
