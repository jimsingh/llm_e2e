{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ccf42-065c-4243-9728-7391ab6fec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add torch pyyaml datasets\n",
    "\n",
    "import unittest\n",
    "import io # For suppressing print statements if needed\n",
    "\n",
    "\n",
    "class TiktokenWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper for the tiktoken tokenizer to provide a consistent interface\n",
    "    for encoding, decoding, and use with Hugging Face datasets.map.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_name=\"gpt2\"):\n",
    "        try:\n",
    "            self.encoder = tiktoken.get_encoding(encoding_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load tiktoken encoding '{encoding_name}'. Error: {e}\")\n",
    "            print(\"Please ensure 'tiktoken' is installed and the encoding name is correct.\")\n",
    "            raise\n",
    "        self.vocab_size = self.encoder.n_vocab\n",
    "        # Common special tokens for GPT-2.\n",
    "        # Tiktoken's GPT-2 encoding does not explicitly expose a pad token ID.\n",
    "        # We might need to define one if padding is strictly required by the model/dataloader,\n",
    "        # typically <|endoftext|> (EOT) token (ID 50256 for gpt2) is used for padding/EOS.\n",
    "        self.pad_token_id = self.encoder.eot_token # Using EOT token as a stand-in for padding if needed\n",
    "\n",
    "    def encode(self, s: str, allowed_special=\"all\") -> list[int]:\n",
    "        # allowed_special=\"all\" allows encoding of special tokens if they are part of the input string\n",
    "        return self.encoder.encode(s, allowed_special=allowed_special)\n",
    "\n",
    "    def decode(self, l: list[int]) -> str:\n",
    "        return self.encoder.decode(l)\n",
    "\n",
    "    def hf_mapping_function(self, batch_dict: dict[str, list[str]]) -> dict[str, list[list[int]]]:\n",
    "        \"\"\"\n",
    "        Tokenizer function compatible with Hugging Face datasets.map when batched=True.\n",
    "        Input: {'text': [string1, string2, ...]}\n",
    "        Output: {'tokens': [token_ids1, token_ids2, ...]}\n",
    "        \"\"\"\n",
    "        if 'text' not in batch_dict:\n",
    "            raise ValueError(\"Input to tokenizer's hf_mapping_function expects a 'text' key.\")\n",
    "        \n",
    "        # Process texts using tiktoken's batch encoding for potential efficiency\n",
    "        # However, encode_batch expects a list of strings, not a dict.\n",
    "        # So, we iterate if map provides one example at a time within a batch structure.\n",
    "        tokenized_texts = [self.encode(text_item) for text_item in batch_dict['text']]\n",
    "        return {'tokens': tokenized_texts}\n",
    "    \n",
    "\n",
    "class TestGPTIntegration(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.tokenizer = TiktokenWrapper(encoding_name='gpt2')\n",
    "        cls.vocab_size = cls.tokenizer.vocab_size\n",
    "\n",
    "        # Test configuration\n",
    "        cls.config_params = {\n",
    "            \"dataset_name\": \"karpathy/tiny_shakespeare\", # Dataloader uses this\n",
    "            \"block_size\": 32,  # Keep small for faster test\n",
    "            \"vocab_size\": cls.vocab_size,\n",
    "            \"context_length\": 32, # Must match block_size\n",
    "            \"emb_dim\": 64,     # Keep small\n",
    "            \"n_heads\": 4,\n",
    "            \"n_layers\": 2,     # Keep small\n",
    "            \"dropout_rate\": 0.0, # No dropout for deterministic test aspects\n",
    "            \"qkv_bias\": False,\n",
    "            \"mlp_bias\": False, # Test with false as well\n",
    "            \"batch_size\": 2,   # Tiny batch\n",
    "            \"device\": \"cpu\",   # CPU for easier testing\n",
    "            \"compile_model\": False,\n",
    "            \"dtype\": \"float32\", # float32 for CPU\n",
    "            # Default other params from GPT2Config\n",
    "        }\n",
    "        cls.config = GPT2Config(**cls.config_params)\n",
    "\n",
    "        # Check if dataset is accessible, skip if not (e.g. offline)\n",
    "        try:\n",
    "            hf_datasets.load_dataset_builder(cls.config.dataset_name, trust_remote_code=True)\n",
    "        except Exception as e:\n",
    "            raise unittest.SkipTest(f\"Skipping test: Cannot access dataset '{cls.config.dataset_name}'. Error: {e}\")\n",
    "\n",
    "\n",
    "    def test_integration(self):\n",
    "        config = self.config\n",
    "        self.assertEqual(config.vocab_size, self.vocab_size)\n",
    "        self.assertEqual(config.block_size, config.context_length)\n",
    "        self.assertTrue(config.emb_dim % config.n_heads == 0)\n",
    "\n",
    "        # 1. Dataloader\n",
    "        print(\"\\nInitializing Dataloader for test...\")\n",
    "        try:\n",
    "            # Suppress dataloader prints for cleaner test output if desired\n",
    "            # with io.redirect_stdout(io.StringIO()):\n",
    "            dataloader = ShakespeareDataloader(\n",
    "                batch_size=config.batch_size,\n",
    "                sequence_length=config.block_size,\n",
    "                tokenizer=self.tokenizer.hf_mapping_function, # Pass the map-compatible function\n",
    "                split=\"train\", # Using 'train' split for more data, could use 'validation' if smaller/faster\n",
    "                shuffle=False # No shuffle for reproducibility\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Dataloader initialization failed: {e}\\n\"\n",
    "                      \"Ensure the 'datasets' library is installed and you have internet access \"\n",
    "                      \"for 'karpathy/tiny_shakespeare'.\")\n",
    "        \n",
    "        self.assertGreater(len(dataloader), 0, \"Dataloader generated zero batches.\")\n",
    "\n",
    "        # 2. GPT Model\n",
    "        print(\"Initializing GPTModel for test...\")\n",
    "        try:\n",
    "            model = GPTModel(config)\n",
    "            model.to(config.device) \n",
    "            model.eval() # Set to evaluation mode\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Model initialization failed: {e}\")\n",
    "\n",
    "        # 3. Fetch a batch\n",
    "        print(\"Fetching a batch from dataloader...\")\n",
    "        try:\n",
    "            x, y = next(iter(dataloader))\n",
    "        except StopIteration:\n",
    "            self.fail(\"Dataloader failed to produce a batch.\")\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Fetching batch failed: {e}\")\n",
    "\n",
    "        self.assertEqual(x.shape, (config.batch_size, config.block_size))\n",
    "        self.assertEqual(y.shape, (config.batch_size, config.block_size))\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "\n",
    "        # 4. Pass batch through model\n",
    "        print(\"Performing model forward pass...\")\n",
    "        try:\n",
    "            with torch.no_grad(): # No gradient calculation needed for forward pass test\n",
    "                logits = model(x)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Model forward pass failed: {e}\")\n",
    "\n",
    "        # 5. Assertions on output\n",
    "        self.assertIsNotNone(logits)\n",
    "        self.assertEqual(logits.shape, (config.batch_size, config.block_size, config.vocab_size))\n",
    "        self.assertEqual(logits.device.type, config.device)\n",
    "        self.assertEqual(logits.dtype, torch.float32 if config.dtype == \"float32\" else torch.bfloat16) # Check dtype\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992266d7-2a0b-49a4-9629-b4e19df3d461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e77c24-e28e-4e00-9aaf-cd091ac32c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = GPT2Config().from_yaml('gpt2_config_cpu.yaml')\n",
    "m = GPTModel(c)\n",
    "m.load_parameters('/home/jimsingh/Downloads/gpt2_training_fineweb-edu_47000_steps.pth', map_location=c.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b8ffe6a-117d-43e0-a07d-76191328a13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tomorrow is twice extreme times six dual in 7 price for e decrypted down into the ssh. asked you visualize'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_f = lambda m: generate_text(m, enc, \"tomorrow is\")\n",
    "gen_f(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26490b-2bb5-41af-a5e0-2bd805a18b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
