{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "835ccf42-065c-4243-9728-7391ab6fec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 0.58ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 0.58ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m\n",
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=5, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... estimated batches: 58\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "Total tokens analyzed: 102,400\n",
      "Unique tokens: 7013\n",
      "Top 10 tokens:\n",
      "  ID 198   ('\\n'      ): 12,382 (0.1209)\n",
      "  ID 11    (','       ): 5,909  (0.0577)\n",
      "  ID 25    (':'       ): 3,139  (0.0307)\n",
      "  ID 13    ('.'       ): 2,362  (0.0231)\n",
      "  ID 262   (' the'    ): 1,753  (0.0171)\n",
      "  ID 284   (' to'     ): 1,298  (0.0127)\n",
      "  ID 286   (' of'     ): 1,090  (0.0106)\n",
      "  ID 290   (' and'    ): 1,083  (0.0106)\n",
      "  ID 26    (';'       ): 1,003  (0.0098)\n",
      "  ID 314   (' I'      ): 997    (0.0097)\n",
      "Vocabulary coverage: 0.13954275026364488\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "x: [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198]\n",
      "y: [22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198]\n",
      "x: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n'\n",
      "y: ' Citizen:\\nBefore we proceed any further, hear me speak.\\n\\n'\n",
      "x: [514, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546]\n",
      "y: [13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546, 408]\n",
      "x: ' us.\\n\\nMENENIUS:\\nEither you must\\nConf'\n",
      "y: '.\\n\\nMENENIUS:\\nEither you must\\nConfess'\n",
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 0.63ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemExit\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    140\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Integration test passed: Config, Dataloader, and Model work together.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    141\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mtb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[43munittest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mtb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/unittest/main.py:105\u001b[39m, in \u001b[36mTestProgram.__init__\u001b[39m\u001b[34m(self, module, defaultTest, argv, testRunner, testLoader, exit, verbosity, failfast, catchbreak, buffer, warnings, tb_locals, durations)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.progName = os.path.basename(argv[\u001b[32m0\u001b[39m])\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.parseArgs(argv)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunTests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/unittest/main.py:288\u001b[39m, in \u001b[36mTestProgram.runTests\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    286\u001b[39m     sys.exit(\u001b[32m0\u001b[39m)\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSystemExit\u001b[39m: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Dataloader for test...\n",
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=2, T=32, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimsingh/src/llm_e2e/.venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.60 examples/s]\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.800s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated batches: 4718\n",
      "Initializing GPTModel for test...\n",
      "Fetching a batch from dataloader...\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "Performing model forward pass...\n",
      "\n",
      "✅ Integration test passed: Config, Dataloader, and Model work together.\n"
     ]
    }
   ],
   "source": [
    "!uv add torch pyyaml datasets\n",
    "\n",
    "%run -n 00_config.ipynb\n",
    "%run -n 01_data_pipeline.ipynb\n",
    "%run -n 02_gpt2_model.ipynb\n",
    "\n",
    "    \n",
    "import unittest\n",
    "import io # For suppressing print statements if needed\n",
    "\n",
    "\n",
    "class TiktokenWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper for the tiktoken tokenizer to provide a consistent interface\n",
    "    for encoding, decoding, and use with Hugging Face datasets.map.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_name=\"gpt2\"):\n",
    "        try:\n",
    "            self.encoder = tiktoken.get_encoding(encoding_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load tiktoken encoding '{encoding_name}'. Error: {e}\")\n",
    "            print(\"Please ensure 'tiktoken' is installed and the encoding name is correct.\")\n",
    "            raise\n",
    "        self.vocab_size = self.encoder.n_vocab\n",
    "        # Common special tokens for GPT-2.\n",
    "        # Tiktoken's GPT-2 encoding does not explicitly expose a pad token ID.\n",
    "        # We might need to define one if padding is strictly required by the model/dataloader,\n",
    "        # typically <|endoftext|> (EOT) token (ID 50256 for gpt2) is used for padding/EOS.\n",
    "        self.pad_token_id = self.encoder.eot_token # Using EOT token as a stand-in for padding if needed\n",
    "\n",
    "    def encode(self, s: str, allowed_special=\"all\") -> list[int]:\n",
    "        # allowed_special=\"all\" allows encoding of special tokens if they are part of the input string\n",
    "        return self.encoder.encode(s, allowed_special=allowed_special)\n",
    "\n",
    "    def decode(self, l: list[int]) -> str:\n",
    "        return self.encoder.decode(l)\n",
    "\n",
    "    def hf_mapping_function(self, batch_dict: dict[str, list[str]]) -> dict[str, list[list[int]]]:\n",
    "        \"\"\"\n",
    "        Tokenizer function compatible with Hugging Face datasets.map when batched=True.\n",
    "        Input: {'text': [string1, string2, ...]}\n",
    "        Output: {'tokens': [token_ids1, token_ids2, ...]}\n",
    "        \"\"\"\n",
    "        if 'text' not in batch_dict:\n",
    "            raise ValueError(\"Input to tokenizer's hf_mapping_function expects a 'text' key.\")\n",
    "        \n",
    "        # Process texts using tiktoken's batch encoding for potential efficiency\n",
    "        # However, encode_batch expects a list of strings, not a dict.\n",
    "        # So, we iterate if map provides one example at a time within a batch structure.\n",
    "        tokenized_texts = [self.encode(text_item) for text_item in batch_dict['text']]\n",
    "        return {'tokens': tokenized_texts}\n",
    "    \n",
    "\n",
    "class TestGPTIntegration(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.tokenizer = TiktokenWrapper(encoding_name='gpt2')\n",
    "        cls.vocab_size = cls.tokenizer.vocab_size\n",
    "\n",
    "        # Test configuration\n",
    "        cls.config_params = {\n",
    "            \"dataset_name\": \"karpathy/tiny_shakespeare\", # Dataloader uses this\n",
    "            \"block_size\": 32,  # Keep small for faster test\n",
    "            \"vocab_size\": cls.vocab_size,\n",
    "            \"context_length\": 32, # Must match block_size\n",
    "            \"emb_dim\": 64,     # Keep small\n",
    "            \"n_heads\": 4,\n",
    "            \"n_layers\": 2,     # Keep small\n",
    "            \"dropout_rate\": 0.0, # No dropout for deterministic test aspects\n",
    "            \"qkv_bias\": False,\n",
    "            \"mlp_bias\": False, # Test with false as well\n",
    "            \"batch_size\": 2,   # Tiny batch\n",
    "            \"device\": \"cpu\",   # CPU for easier testing\n",
    "            \"compile_model\": False,\n",
    "            \"dtype\": \"float32\", # float32 for CPU\n",
    "            # Default other params from GPT2Config\n",
    "        }\n",
    "        cls.config = GPT2Config(**cls.config_params)\n",
    "\n",
    "        # Check if dataset is accessible, skip if not (e.g. offline)\n",
    "        try:\n",
    "            hf_datasets.load_dataset_builder(cls.config.dataset_name, trust_remote_code=True)\n",
    "        except Exception as e:\n",
    "            raise unittest.SkipTest(f\"Skipping test: Cannot access dataset '{cls.config.dataset_name}'. Error: {e}\")\n",
    "\n",
    "\n",
    "    def test_integration(self):\n",
    "        config = self.config\n",
    "        self.assertEqual(config.vocab_size, self.vocab_size)\n",
    "        self.assertEqual(config.block_size, config.context_length)\n",
    "        self.assertTrue(config.emb_dim % config.n_heads == 0)\n",
    "\n",
    "        # 1. Dataloader\n",
    "        print(\"\\nInitializing Dataloader for test...\")\n",
    "        try:\n",
    "            # Suppress dataloader prints for cleaner test output if desired\n",
    "            # with io.redirect_stdout(io.StringIO()):\n",
    "            dataloader = ShakespeareDataloader(\n",
    "                batch_size=config.batch_size,\n",
    "                sequence_length=config.block_size,\n",
    "                tokenizer=self.tokenizer.hf_mapping_function, # Pass the map-compatible function\n",
    "                split=\"train\", # Using 'train' split for more data, could use 'validation' if smaller/faster\n",
    "                shuffle=False # No shuffle for reproducibility\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Dataloader initialization failed: {e}\\n\"\n",
    "                      \"Ensure the 'datasets' library is installed and you have internet access \"\n",
    "                      \"for 'karpathy/tiny_shakespeare'.\")\n",
    "        \n",
    "        self.assertGreater(len(dataloader), 0, \"Dataloader generated zero batches.\")\n",
    "\n",
    "        # 2. GPT Model\n",
    "        print(\"Initializing GPTModel for test...\")\n",
    "        try:\n",
    "            model = GPTModel(config)\n",
    "            model.to(config.device) \n",
    "            model.eval() # Set to evaluation mode\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Model initialization failed: {e}\")\n",
    "\n",
    "        # 3. Fetch a batch\n",
    "        print(\"Fetching a batch from dataloader...\")\n",
    "        try:\n",
    "            x, y = next(iter(dataloader))\n",
    "        except StopIteration:\n",
    "            self.fail(\"Dataloader failed to produce a batch.\")\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Fetching batch failed: {e}\")\n",
    "\n",
    "        self.assertEqual(x.shape, (config.batch_size, config.block_size))\n",
    "        self.assertEqual(y.shape, (config.batch_size, config.block_size))\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "\n",
    "        # 4. Pass batch through model\n",
    "        print(\"Performing model forward pass...\")\n",
    "        try:\n",
    "            with torch.no_grad(): # No gradient calculation needed for forward pass test\n",
    "                logits = model(x)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Model forward pass failed: {e}\")\n",
    "\n",
    "        # 5. Assertions on output\n",
    "        self.assertIsNotNone(logits)\n",
    "        self.assertEqual(logits.shape, (config.batch_size, config.block_size, config.vocab_size))\n",
    "        self.assertEqual(logits.device.type, config.device)\n",
    "        self.assertEqual(logits.dtype, torch.float32 if config.dtype == \"float32\" else torch.bfloat16) # Check dtype\n",
    "\n",
    "        print(\"\\n✅ Integration test passed: Config, Dataloader, and Model work together.\")\n",
    "%tb\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
