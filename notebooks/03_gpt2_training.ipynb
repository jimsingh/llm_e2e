{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad511bae-86d5-4b9d-a0ac-bed20db8e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m169 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m165 packages\u001b[0m \u001b[2min 0.22ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rahular/simple-wikipedia couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/jimsingh/.cache/huggingface/datasets/rahular___simple-wikipedia/default/0.0.0/b716115a07479d49e58233d2382295c5ca585431 (last modified on Fri Jun  6 23:30:01 2025).\n"
     ]
    }
   ],
   "source": [
    "%run -n 00_config.ipynb\n",
    "%run -n 01_data_pipeline.ipynb\n",
    "%run -n 02_gpt2_model.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e5f906-5761-44e9-85a4-009c8299c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt: str, max_tokens=20) -> str:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    encoded_ids = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        output_token_ids = model.generate(encoded_ids, max_tokens)\n",
    "    \n",
    "    decoded_ids_list = output_token_ids[0].cpu().tolist()\n",
    "    decoded_text = tokenizer.decode(decoded_ids_list)\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "def estimate_loss(model, loader, device, eval_iters):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for i, (X, Y) in enumerate(itertools.islice(loader, eval_iters)):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "@torch.no_grad\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: to evaluate\n",
    "      train_loader: training dataset iterator\n",
    "      val_loader: validation dataset iterator\n",
    "      eval_iters: the number of iterations to pull from the loaders\n",
    "\n",
    "    Returns:\n",
    "      dict with 'train' and 'val' loss\n",
    "  \"\"\"\n",
    "    train_loss = estimate_loss(model, train_loader, device, eval_iters)\n",
    "    val_loss = estimate_loss(model, val_loader, device, eval_iters)\n",
    "    return {'train': train_loss, 'val': val_loss}\n",
    "\n",
    "def print_gpu_memory_stats(checkpoint_name, device):\n",
    "    if torch.cuda.is_available() and device.type == 'cuda':\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        reserved = torch.cuda.memory_reserved(device) / (1024**2)\n",
    "        max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "        max_reserved = torch.cuda.max_memory_reserved(device) / (1024**2)\n",
    "\n",
    "        print(f\"--- GPU Memory Stats at: {checkpoint_name} ({device}) ---\")\n",
    "        print(f\"  Current Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"  Current Reserved:  {reserved:.2f} MB\")\n",
    "        print(f\"  Peak Allocated:    {max_allocated:.2f} MB\")\n",
    "        print(f\"  Peak Reserved:     {max_reserved:.2f} MB\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        \n",
    "def train_model(model, train_loader, val_loader, optimizer, gen_f, cfg):\n",
    "    device = torch.device(cfg.device) # Ensure device object\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        print(\"Starting training on CUDA device. Initializing memory stats.\")\n",
    "        # Reset peak stats at the beginning of training if you want to track peaks per training run\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        print_gpu_memory_stats(\"Start of training_model\", device)\n",
    "    \n",
    "    print(f\"started training model with {cfg.n_params:_} parameters. model parameters file: {cfg.save_filename}\")\n",
    "    \n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"[{epoch + 1} / {cfg.num_epochs}]: starting at {datetime.now()}, will show running loss every {cfg.log_interval} steps, will eval every {cfg.eval_interval} steps\")\n",
    "        if device.type == 'cuda':\n",
    "            print_gpu_memory_stats(f\"Start of Epoch {epoch + 1}\", device)\n",
    "\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "            X, Y = X.to(cfg.device), Y.to(cfg.device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            logits, loss = model(X, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % cfg.log_interval == 0:\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: running loss {running_loss / cfg.log_interval:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if (i + 1) % cfg.eval_interval == 0:\n",
    "                losses = evaluate_model(model, train_loader, val_loader, device, eval_iters=cfg.eval_iters)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}, eval_iters: {cfg.eval_iters}\")\n",
    "                completion = gen_f(model)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: {completion}\")\n",
    "                print_gpu_memory_stats(f\"[{epoch + 1}  {i + 1:5d}]\", device)\n",
    "                torch.save(model, cfg.save_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded config from: gpt2_config_wikipedia_cpu.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rahular/simple-wikipedia couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/jimsingh/.cache/huggingface/datasets/rahular___simple-wikipedia/default/0.0.0/b716115a07479d49e58233d2382295c5ca585431 (last modified on Fri Jun  6 23:30:01 2025).\n",
      "Using the latest cached version of the dataset since rahular/simple-wikipedia couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/jimsingh/.cache/huggingface/datasets/rahular___simple-wikipedia/default/0.0.0/b716115a07479d49e58233d2382295c5ca585431 (last modified on Fri Jun  6 23:30:01 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training model with 22_939_776 parameters. checkpoint file: rahular_simple-wikipedia.22939776.pt\n",
      "[1 / 20]: starting at 2025-06-07 11:13:26.338452, will show running loss every 10 steps, will eval every 50 steps\n",
      "[1     10]: running loss 5.528\n",
      "[1     20]: running loss 5.439\n",
      "[1     30]: running loss 5.471\n",
      "[1     40]: running loss 5.427\n",
      "[1     50]: running loss 5.395\n",
      "[1     50]: train loss: 5.1600, val loss: 4.9579, eval_iters: 10\n",
      "[1     50]: it was the happiest.\n",
      "[1     60]: running loss 5.427\n"
     ]
    }
   ],
   "source": [
    "cfg = GPT2Config().from_yaml(\"gpt2_config_wikipedia_cpu.yaml\")\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "model = GPTModel(cfg)\n",
    "\n",
    "load_full_model = True\n",
    "load_weights = False\n",
    "if load_full_model:\n",
    "    model = torch.load(cfg.save_filename, weights_only=False)\n",
    "elif load_weights:\n",
    "    # Create a new state_dict without the \"_orig_mod.\" prefix\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_key = k[len(\"_orig_mod.\"):]  # Remove the prefix\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v # If for some reason some keys don't have it\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "gen_f = lambda m: generate_text(m, enc, \"it was the happiest\")\n",
    "\n",
    "train_loader = GeneratorWrapper(cfg, enc)\n",
    "val_loader = GeneratorWrapper(cfg, enc)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    assert cfg.device == 'cuda'\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    capability = torch.cTrueuda.get_device_capability()\n",
    "    if capability[0] >= 7:  # Volta (7.0+), Turing (7.5+), Ampere (8.0+), Hopper (9.0+)\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        print(\"Uses tensor cores\")\n",
    "    else:\n",
    "        print(\"Tensor cores not supported on this GPU. Using default precision.\")\n",
    "    print(f\"Uses tensor cores: {torch.cuda.is_available()}\")\n",
    "\n",
    "    model.to(torch.bfloat16)\n",
    "\n",
    "model.to(cfg.device)\n",
    "if cfg.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "train_model(model, train_loader, val_loader, optimizer, gen_f, cfg=cfg)\n",
    "\n",
    "torch.save(model, cfg.save_filename)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6362a263-cefd-4f8f-9eb1-93b1a42e68fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the saved state_dict\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     cfg = \u001b[43mGPT2Config\u001b[49m().from_yaml(\u001b[33m\"\u001b[39m\u001b[33mgpt2_config_wikipedia_cpu.yaml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     enc = tiktoken.get_encoding(cfg.encoding_name)\n\u001b[32m      9\u001b[39m     state_dict = torch.load(cfg.save_filename)\n",
      "\u001b[31mNameError\u001b[39m: name 'GPT2Config' is not defined"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    model_path = 'gpt2_training_fineweb-edu_47000_steps.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "# Load the saved state_dict\n",
    "if True:\n",
    "    # Instantiate the model\n",
    "    cfg = GPT2Config().from_yaml(\"gpt2_config_wikipedia_cpu.yaml\")\n",
    "    enc = tiktoken.get_encoding(cfg.encoding_name)\n",
    "    state_dict = torch.load(cfg.save_filename)\n",
    "    \n",
    "    # Create a new state_dict without the \"_orig_mod.\" prefix\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_key = k[len(\"_orig_mod.\"):]  # Remove the prefix\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v # If for some reason some keys don't have it\n",
    "    \n",
    "    model = GPTModel(cfg)\n",
    "    model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623b090-c097-4b8d-a040-4704e6e213a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_gpu_memory_stats('foo', torch.device(cfg.device))\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "# Assuming 'my_lingering_tensor_var' is a variable holding a tensor\n",
    "# or 'my_list_of_tensors' is a list holding them.\n",
    "\n",
    "# If you know the variable names:\n",
    "# del my_lingering_tensor_var\n",
    "# del my_list_of_tensors\n",
    "\n",
    "# Then run garbage collection\n",
    "gc.collect()\n",
    "if False:\n",
    "  del model, optimizer, train_loader, val_loader\n",
    "\n",
    "gc.collect()\n",
    "# It's also good practice to clear PyTorch's cache AFTER Python references are gone\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
