{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ad511bae-86d5-4b9d-a0ac-bed20db8e160",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad511bae-86d5-4b9d-a0ac-bed20db8e160",
        "outputId": "58503160-5068-46ce-e3d0-8410e10e5518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setting up colab environment...\n",
            "installing dependencies with uv...\n",
            "warning (uv sync): Using CPython 3.11.13 interpreter at: /usr/bin/python3\n",
            "Creating virtual environment at: .venv\n",
            "Resolved 193 packages in 24ms\n",
            "   Building llm-e2e @ file:///content/llm_e2e\n",
            "Downloading nvidia-cusolver-cu12 (150.9MiB)\n",
            "Downloading nvidia-cuda-nvrtc-cu12 (22.6MiB)\n",
            "Downloading tensorboard (5.2MiB)\n",
            "Downloading matplotlib (8.2MiB)\n",
            "Downloading fonttools (4.7MiB)\n",
            "Downloading pillow (4.4MiB)\n",
            "Downloading networkx (1.9MiB)\n",
            "Downloading jedi (1.5MiB)\n",
            "Downloading keras (1.3MiB)\n",
            "Downloading setuptools (1.1MiB)\n",
            "Downloading tiktoken (1.1MiB)\n",
            "Downloading nvidia-cufile-cu12 (1.1MiB)\n",
            "Downloading pyarrow (40.3MiB)\n",
            "Downloading kiwisolver (1.4MiB)\n",
            "Downloading nvidia-cudnn-cu12 (544.5MiB)\n",
            "Downloading nvidia-cublas-cu12 (374.9MiB)\n",
            "Downloading libclang (23.4MiB)\n",
            "Downloading sympy (6.0MiB)\n",
            "Downloading nvidia-nvjitlink-cu12 (18.8MiB)\n",
            "Downloading nvidia-cusparse-cu12 (206.5MiB)\n",
            "Downloading pygments (1.2MiB)\n",
            "Downloading babel (9.7MiB)\n",
            "Downloading nvidia-curand-cu12 (53.7MiB)\n",
            "Downloading nvidia-cufft-cu12 (190.9MiB)\n",
            "Downloading nvidia-nccl-cu12 (192.0MiB)\n",
            "Downloading aiohttp (1.6MiB)\n",
            "Downloading nvidia-cusparselt-cu12 (149.5MiB)\n",
            "Downloading tensorflow (615.0MiB)\n",
            "Downloading numpy (15.6MiB)\n",
            "Downloading torch (825.1MiB)\n",
            "Downloading pydantic-core (1.9MiB)\n",
            "Downloading triton (149.3MiB)\n",
            "Downloading tensorflow-io-gcs-filesystem (4.9MiB)\n",
            "Downloading wandb (22.1MiB)\n",
            "Downloading hf-xet (4.9MiB)\n",
            "Downloading h5py (4.3MiB)\n",
            "Downloading nvidia-cuda-cupti-cu12 (8.5MiB)\n",
            "Downloading tensorboard-data-server (6.3MiB)\n",
            "Downloading grpcio (5.6MiB)\n",
            "Downloading pandas (12.5MiB)\n",
            "Downloading jupyterlab (11.7MiB)\n",
            "Downloading ml-dtypes (4.5MiB)\n",
            "Downloading debugpy (3.0MiB)\n",
            " Downloaded nvidia-cufile-cu12\n",
            " Downloaded tiktoken\n",
            " Downloaded kiwisolver\n",
            " Downloaded pygments\n",
            " Downloaded aiohttp\n",
            " Downloaded pydantic-core\n",
            " Downloaded setuptools\n",
            " Downloaded networkx\n",
            " Downloaded keras\n",
            " Downloaded debugpy\n",
            "      Built llm-e2e @ file:///content/llm_e2e\n",
            " Downloaded ml-dtypes\n",
            " Downloaded h5py\n",
            " Downloaded pillow\n",
            " Downloaded fonttools\n",
            " Downloaded tensorflow-io-gcs-filesystem\n",
            " Downloaded hf-xet\n",
            " Downloaded tensorboard\n",
            " Downloaded grpcio\n",
            " Downloaded tensorboard-data-server\n",
            " Downloaded nvidia-cuda-cupti-cu12\n",
            " Downloaded matplotlib\n",
            " Downloaded sympy\n",
            " Downloaded babel\n",
            " Downloaded pandas\n",
            " Downloaded numpy\n",
            " Downloaded nvidia-nvjitlink-cu12\n",
            " Downloaded nvidia-cuda-nvrtc-cu12\n",
            " Downloaded libclang\n",
            " Downloaded jupyterlab\n",
            " Downloaded jedi\n",
            " Downloaded nvidia-curand-cu12\n",
            " Downloaded wandb\n",
            " Downloaded pyarrow\n",
            " Downloaded nvidia-cusparselt-cu12\n",
            " Downloaded nvidia-cusolver-cu12\n",
            " Downloaded nvidia-cufft-cu12\n",
            " Downloaded nvidia-nccl-cu12\n",
            " Downloaded triton\n",
            " Downloaded nvidia-cusparse-cu12\n",
            " Downloaded nvidia-cublas-cu12\n",
            " Downloaded nvidia-cudnn-cu12\n",
            " Downloaded torch\n",
            " Downloaded tensorflow\n",
            "Prepared 188 packages in 1m 02s\n",
            "Installed 188 packages in 481ms\n",
            " + absl-py==2.3.0\n",
            " + aiohappyeyeballs==2.6.1\n",
            " + aiohttp==3.12.4\n",
            " + aiosignal==1.3.2\n",
            " + annotated-types==0.7.0\n",
            " + anyio==4.9.0\n",
            " + argon2-cffi==23.1.0\n",
            " + argon2-cffi-bindings==21.2.0\n",
            " + arrow==1.3.0\n",
            " + asttokens==3.0.0\n",
            " + astunparse==1.6.3\n",
            " + async-lru==2.0.5\n",
            " + attrs==25.3.0\n",
            " + babel==2.17.0\n",
            " + beautifulsoup4==4.13.4\n",
            " + bleach==6.2.0\n",
            " + certifi==2025.4.26\n",
            " + cffi==1.17.1\n",
            " + charset-normalizer==3.4.2\n",
            " + click==8.2.1\n",
            " + comm==0.2.2\n",
            " + commonmark==0.9.1\n",
            " + contourpy==1.3.2\n",
            " + coverage==7.8.2\n",
            " + cycler==0.12.1\n",
            " + datasets==3.6.0\n",
            " + debugpy==1.8.14\n",
            " + decorator==5.2.1\n",
            " + defusedxml==0.7.1\n",
            " + dill==0.3.8\n",
            " + docutils==0.21.2\n",
            " + einops==0.8.1\n",
            " + executing==2.2.0\n",
            " + fastjsonschema==2.21.1\n",
            " + filelock==3.18.0\n",
            " + flatbuffers==25.2.10\n",
            " + fonttools==4.58.2\n",
            " + fqdn==1.5.1\n",
            " + frozenlist==1.6.0\n",
            " + fsspec==2025.3.0\n",
            " + gast==0.6.0\n",
            " + gitdb==4.0.12\n",
            " + gitpython==3.1.44\n",
            " + google-pasta==0.2.0\n",
            " + grpcio==1.72.1\n",
            " + h11==0.16.0\n",
            " + h5py==3.13.0\n",
            " + hf-xet==1.1.2\n",
            " + httpcore==1.0.9\n",
            " + httpx==0.28.1\n",
            " + huggingface-hub==0.32.3\n",
            " + idna==3.10\n",
            " + iniconfig==2.1.0\n",
            " + ipykernel==6.29.5\n",
            " + ipython==9.2.0\n",
            " + ipython-pygments-lexers==1.1.1\n",
            " + isoduration==20.11.0\n",
            " + jedi==0.19.2\n",
            " + jinja2==3.1.6\n",
            " + json5==0.12.0\n",
            " + jsonpointer==3.0.0\n",
            " + jsonschema==4.24.0\n",
            " + jsonschema-specifications==2025.4.1\n",
            " + jupyter-client==8.6.3\n",
            " + jupyter-core==5.8.1\n",
            " + jupyter-events==0.12.0\n",
            " + jupyter-lsp==2.2.5\n",
            " + jupyter-server==2.16.0\n",
            " + jupyter-server-terminals==0.5.3\n",
            " + jupyterlab==4.4.3\n",
            " + jupyterlab-pygments==0.3.0\n",
            " + jupyterlab-server==2.27.3\n",
            " + keras==3.10.0\n",
            " + kiwisolver==1.4.8\n",
            " + libclang==18.1.1\n",
            " + llm-e2e==0.1.0 (from file:///content/llm_e2e)\n",
            " + markdown==3.8\n",
            " + markupsafe==3.0.2\n",
            " + matplotlib==3.10.3\n",
            " + matplotlib-inline==0.1.7\n",
            " + mistune==3.1.3\n",
            " + ml-dtypes==0.5.1\n",
            " + mpmath==1.3.0\n",
            " + multidict==6.4.4\n",
            " + multiprocess==0.70.16\n",
            " + namex==0.1.0\n",
            " + nbclient==0.10.2\n",
            " + nbconvert==7.16.6\n",
            " + nbformat==5.10.4\n",
            " + nest-asyncio==1.6.0\n",
            " + networkx==3.5\n",
            " + notebook-shim==0.2.4\n",
            " + numpy==2.1.3\n",
            " + nvidia-cublas-cu12==12.6.4.1\n",
            " + nvidia-cuda-cupti-cu12==12.6.80\n",
            " + nvidia-cuda-nvrtc-cu12==12.6.77\n",
            " + nvidia-cuda-runtime-cu12==12.6.77\n",
            " + nvidia-cudnn-cu12==9.5.1.17\n",
            " + nvidia-cufft-cu12==11.3.0.4\n",
            " + nvidia-cufile-cu12==1.11.1.6\n",
            " + nvidia-curand-cu12==10.3.7.77\n",
            " + nvidia-cusolver-cu12==11.7.1.2\n",
            " + nvidia-cusparse-cu12==12.5.4.2\n",
            " + nvidia-cusparselt-cu12==0.6.3\n",
            " + nvidia-nccl-cu12==2.26.2\n",
            " + nvidia-nvjitlink-cu12==12.6.85\n",
            " + nvidia-nvtx-cu12==12.6.77\n",
            " + opt-einsum==3.4.0\n",
            " + optree==0.16.0\n",
            " + overrides==7.7.0\n",
            " + packaging==25.0\n",
            " + pandas==2.2.3\n",
            " + pandocfilters==1.5.1\n",
            " + parso==0.8.4\n",
            " + pexpect==4.9.0\n",
            " + pillow==11.2.1\n",
            " + platformdirs==4.3.8\n",
            " + pluggy==1.6.0\n",
            " + prometheus-client==0.22.0\n",
            " + prompt-toolkit==3.0.51\n",
            " + propcache==0.3.1\n",
            " + protobuf==5.29.5\n",
            " + psutil==7.0.0\n",
            " + ptyprocess==0.7.0\n",
            " + pure-eval==0.2.3\n",
            " + pyarrow==20.0.0\n",
            " + pycparser==2.22\n",
            " + pydantic==2.11.5\n",
            " + pydantic-core==2.33.2\n",
            " + pygments==2.19.1\n",
            " + pyparsing==3.2.3\n",
            " + pytest==8.4.0\n",
            " + pytest-cov==6.1.1\n",
            " + python-dateutil==2.9.0.post0\n",
            " + python-json-logger==3.3.0\n",
            " + pytz==2025.2\n",
            " + pyyaml==6.0.2\n",
            " + pyzmq==26.4.0\n",
            " + referencing==0.36.2\n",
            " + regex==2024.11.6\n",
            " + requests==2.32.3\n",
            " + rfc3339-validator==0.1.4\n",
            " + rfc3986-validator==0.1.1\n",
            " + rich==12.6.0\n",
            " + rich-cli==1.8.0\n",
            " + rich-rst==1.3.1\n",
            " + rpds-py==0.25.1\n",
            " + seaborn==0.13.2\n",
            " + send2trash==1.8.3\n",
            " + sentry-sdk==2.29.1\n",
            " + setproctitle==1.3.6\n",
            " + setuptools==80.9.0\n",
            " + six==1.17.0\n",
            " + smmap==5.0.2\n",
            " + sniffio==1.3.1\n",
            " + soupsieve==2.7\n",
            " + stack-data==0.6.3\n",
            " + sympy==1.14.0\n",
            " + tensorboard==2.19.0\n",
            " + tensorboard-data-server==0.7.2\n",
            " + tensorflow==2.19.0\n",
            " + tensorflow-io-gcs-filesystem==0.37.1\n",
            " + termcolor==3.1.0\n",
            " + terminado==0.18.1\n",
            " + textual==0.1.18\n",
            " + tiktoken==0.9.0\n",
            " + tinycss2==1.4.0\n",
            " + torch==2.7.0\n",
            " + tornado==6.5.1\n",
            " + tqdm==4.67.1\n",
            " + traitlets==5.14.3\n",
            " + triton==3.3.0\n",
            " + types-python-dateutil==2.9.0.20250516\n",
            " + typing-extensions==4.13.2\n",
            " + typing-inspection==0.4.1\n",
            " + tzdata==2025.2\n",
            " + uri-template==1.3.0\n",
            " + urllib3==2.4.0\n",
            " + wandb==0.20.1\n",
            " + wcwidth==0.2.13\n",
            " + webcolors==24.11.1\n",
            " + webencodings==0.5.1\n",
            " + websocket-client==1.8.0\n",
            " + werkzeug==3.1.3\n",
            " + wheel==0.45.1\n",
            " + wrapt==1.17.2\n",
            " + xxhash==3.5.0\n",
            " + yarl==1.20.0\n",
            "warning (uv pip install -e .): Using Python 3.11.13 environment at: /usr\n",
            "Resolved 184 packages in 1.11s\n",
            "   Building llm-e2e @ file:///content/llm_e2e\n",
            "Downloading triton (148.5MiB)\n",
            "Downloading torch (783.1MiB)\n",
            "      Built llm-e2e @ file:///content/llm_e2e\n",
            " Downloaded triton\n",
            " Downloaded torch\n",
            "Prepared 5 packages in 11.82s\n",
            "Uninstalled 26 packages in 1.78s\n",
            "Installed 51 packages in 417ms\n",
            " + arrow==1.3.0\n",
            " + async-lru==2.0.5\n",
            " + commonmark==0.9.1\n",
            " + coverage==7.9.0\n",
            " - datasets==2.14.4\n",
            " + datasets==3.6.0\n",
            " + fqdn==1.5.1\n",
            " - fsspec==2025.3.2\n",
            " + fsspec==2025.3.0\n",
            " + isoduration==20.11.0\n",
            " + jedi==0.19.2\n",
            " + json5==0.12.0\n",
            " - jupyter-client==6.1.12\n",
            " + jupyter-client==8.6.3\n",
            " + jupyter-events==0.12.0\n",
            " + jupyter-lsp==2.2.5\n",
            " - jupyter-server==1.16.0\n",
            " + jupyter-server==2.16.0\n",
            " + jupyter-server-terminals==0.5.3\n",
            " + jupyterlab==4.4.3\n",
            " + jupyterlab-server==2.27.3\n",
            " + llm-e2e==0.1.0 (from file:///content/llm_e2e)\n",
            " - ml-dtypes==0.4.1\n",
            " + ml-dtypes==0.5.1\n",
            " - nvidia-cublas-cu12==12.5.3.2\n",
            " + nvidia-cublas-cu12==12.6.4.1\n",
            " - nvidia-cuda-cupti-cu12==12.5.82\n",
            " + nvidia-cuda-cupti-cu12==12.6.80\n",
            " - nvidia-cuda-nvrtc-cu12==12.5.82\n",
            " + nvidia-cuda-nvrtc-cu12==12.6.77\n",
            " - nvidia-cuda-runtime-cu12==12.5.82\n",
            " + nvidia-cuda-runtime-cu12==12.6.77\n",
            " - nvidia-cudnn-cu12==9.3.0.75\n",
            " + nvidia-cudnn-cu12==9.5.1.17\n",
            " - nvidia-cufft-cu12==11.2.3.61\n",
            " + nvidia-cufft-cu12==11.3.0.4\n",
            " + nvidia-cufile-cu12==1.11.1.6\n",
            " - nvidia-curand-cu12==10.3.6.82\n",
            " + nvidia-curand-cu12==10.3.7.77\n",
            " - nvidia-cusolver-cu12==11.6.3.83\n",
            " + nvidia-cusolver-cu12==11.7.1.2\n",
            " - nvidia-cusparse-cu12==12.5.1.3\n",
            " + nvidia-cusparse-cu12==12.5.4.2\n",
            " - nvidia-cusparselt-cu12==0.6.2\n",
            " + nvidia-cusparselt-cu12==0.6.3\n",
            " - nvidia-nccl-cu12==2.21.5\n",
            " + nvidia-nccl-cu12==2.26.2\n",
            " - nvidia-nvjitlink-cu12==12.5.82\n",
            " + nvidia-nvjitlink-cu12==12.6.85\n",
            " - nvidia-nvtx-cu12==12.4.127\n",
            " + nvidia-nvtx-cu12==12.6.77\n",
            " + overrides==7.7.0\n",
            " - pytest==8.3.5\n",
            " + pytest==8.4.0\n",
            " + pytest-cov==6.2.1\n",
            " + python-json-logger==3.3.0\n",
            " + rfc3339-validator==0.1.4\n",
            " + rfc3986-validator==0.1.1\n",
            " - rich==13.9.4\n",
            " + rich==12.6.0\n",
            " + rich-cli==1.8.0\n",
            " + rich-rst==1.3.1\n",
            " - sympy==1.13.1\n",
            " + sympy==1.14.0\n",
            " - tensorboard==2.18.0\n",
            " + tensorboard==2.19.0\n",
            " - tensorflow==2.18.0\n",
            " + tensorflow==2.19.0\n",
            " + textual==0.1.18\n",
            " - torch==2.6.0+cu124 (from https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl)\n",
            " + torch==2.7.1\n",
            " - triton==3.2.0\n",
            " + triton==3.3.1\n",
            " + types-python-dateutil==2.9.0.20250516\n",
            " + uri-template==1.3.0\n",
            " - wandb==0.19.11\n",
            " + wandb==0.20.1\n",
            "mounting google drive...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import site\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "def in_google_colab() -> bool:\n",
        "    return 'google.colab' in sys.modules\n",
        "\n",
        "def _run_uv_command_quietly(command: list[str], cwd: Path) -> None:\n",
        "    full_command = [\"uv\"] + command\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            full_command,\n",
        "            cwd=cwd,\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            env={\n",
        "                **os.environ,\n",
        "                \"UV_CONSTRAINT\": \"\",\n",
        "                \"UV_BUILD_CONSTRAINT\": \"\",\n",
        "                \"UV_PRERELEASE\": \"if-necessary-or-explicit\"\n",
        "            }\n",
        "        )\n",
        "        if result.stderr:\n",
        "            print(f\"warning (uv {' '.join(command)}): {result.stderr.strip()}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"error: uv command '{' '.join(full_command)}' failed.\")\n",
        "        print(f\"stdout: {e.stdout.strip()}\")\n",
        "        print(f\"stderr: {e.stderr.strip()}\")\n",
        "        raise # re-raise the exception after printing details\n",
        "\n",
        "def _run_git_clone(repo_url: str, branch: str = \"main\", cwd: Path = Path(\".\")) -> None:\n",
        "    # clones a git repository, handling existing directories quietly\n",
        "    repo_name = repo_url.split('/')[-1].removesuffix(\".git\")\n",
        "    destination_path = cwd / repo_name\n",
        "    if os.path.exists(destination_path):\n",
        "      return\n",
        "\n",
        "    git_command = [\"git\", \"clone\", \"--branch\", branch, repo_url, str(destination_path)]\n",
        "\n",
        "    try:\n",
        "        subprocess.run(\n",
        "            git_command,\n",
        "            cwd=cwd,\n",
        "            check=True,\n",
        "            capture_output=True, # captures output but doesn't print it\n",
        "            text=True\n",
        "        )\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"error: git clone command '{' '.join(git_command)}' failed.\")\n",
        "        print(f\"stdout: {e.stdout.strip()}\")\n",
        "        print(f\"stderr: {e.stderr.strip()}\")\n",
        "        raise\n",
        "\n",
        "def setup_colab_environment(project_root: str, package_name: str) -> None:\n",
        "    \"\"\"\n",
        "    sets up the python environment in google colab for a project.\n",
        "    this includes installing dependencies with uv and ensuring the project is importable.\n",
        "    minimal output is produced.\n",
        "\n",
        "    args:\n",
        "        project_root: the absolute path to your project's root directory.\n",
        "                      e.g., '/content/llm_e2e/'\n",
        "        package_name: the top-level name of your python package that should be importable.\n",
        "                      e.g., 'llm_e2e' if you do 'import llm_e2e'\n",
        "    \"\"\"\n",
        "    if not in_google_colab():\n",
        "        return\n",
        "\n",
        "    print(\"setting up colab environment...\")\n",
        "    proj_path = Path(project_root)\n",
        "\n",
        "    _run_git_clone(\"https://github.com/jimsingh/llm_e2e\", cwd=Path('/content/'))\n",
        "\n",
        "    if not (proj_path / \"pyproject.toml\").is_file():\n",
        "        raise FileNotFoundError(f\"'pyproject.toml' not found in {proj_path}. cannot proceed.\")\n",
        "\n",
        "    if str(proj_path) not in sys.path:\n",
        "        sys.path.insert(0, str(proj_path))\n",
        "\n",
        "    print(\"installing dependencies with uv...\")\n",
        "    _run_uv_command_quietly([\"sync\"], cwd=proj_path)\n",
        "    _run_uv_command_quietly([\"pip\", \"install\", \"-e\", \".\"], cwd=proj_path)\n",
        "\n",
        "    if not os.path.exists('/content/drive/MyDrive/llm_e2e/'):\n",
        "        from google.colab import drive\n",
        "        print(\"mounting google drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "    else:\n",
        "        print(\"google drive already mounted.\")\n",
        "\n",
        "\n",
        "def setup_cuda(cfg):\n",
        "    if not torch.cuda.is_available():\n",
        "        return\n",
        "\n",
        "    assert cfg.device == 'cuda', \"cfg.device must be 'cuda' if CUDA is available.\"\n",
        "    print(f\"cuda version: {torch.version.cuda}\")\n",
        "    capability = torch.cuda.get_device_capability()\n",
        "    if capability[0] >= 7:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "        print(\"uses tensor cores\")\n",
        "    else:\n",
        "        print(\"tensor cores not supported on this gpu.\")\n",
        "\n",
        "package_name = 'llm_e2e'\n",
        "project_name = 'llm_e2e'\n",
        "\n",
        "if in_google_colab():\n",
        "  project_root = f\"/content/{project_name}/\"\n",
        "else:\n",
        "  project_root = os.path.expandvars(f\"$HOME/src/{project_name}/\")\n",
        "\n",
        "setup_colab_environment(project_root, package_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebJDH2cZW30d",
      "metadata": {
        "id": "ebJDH2cZW30d"
      },
      "outputs": [],
      "source": [
        "from llm_e2e import GPT2Config, GPT2Model, StreamingDatasetGenerator\n",
        "config_yaml = f\"gpt2_bert_corpus_gpu.yaml\"\n",
        "cfg = GPT2Config.from_yaml(f\"{project_root}/config/{config_yaml}\")\n",
        "encoding = tiktoken.get_encoding(cfg.encoding_name)\n",
        "setup_cuda(cfg)\n",
        "\n",
        "train_dataset = StreamingDatasetGenerator(cfg, encoding=encoding)\n",
        "val_dataset = StreamingDatasetGenerator(cfg, encoding=encoding, split='train', seed=1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e5f906-5761-44e9-85a4-009c8299c200",
      "metadata": {
        "id": "b6e5f906-5761-44e9-85a4-009c8299c200"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "def generate_text(model, tokenizer, prompt: str, max_tokens=20) -> str:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    encoded = tokenizer.encode(prompt)\n",
        "    encoded_ids = torch.tensor([encoded], dtype=torch.long).to(device)\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        output_token_ids = model.generate(encoded_ids, max_tokens)\n",
        "\n",
        "    decoded_ids_list = output_token_ids[0].cpu().tolist()\n",
        "    decoded_text = tokenizer.decode(decoded_ids_list)\n",
        "    return decoded_text\n",
        "\n",
        "def estimate_loss(model, loader, device, eval_iters):\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for i, (X, Y) in enumerate(itertools.islice(loader, eval_iters)):\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[i] = loss.item()\n",
        "    model.train()\n",
        "    return losses.mean()\n",
        "\n",
        "@torch.no_grad\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iters):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      model: to evaluate\n",
        "      train_loader: training dataset iterator\n",
        "      val_loader: validation dataset iterator\n",
        "      eval_iters: the number of iterations to pull from the loaders\n",
        "\n",
        "    Returns:\n",
        "      dict with 'train' and 'val' loss\n",
        "  \"\"\"\n",
        "    train_loss = estimate_loss(model, train_loader, device, eval_iters)\n",
        "    val_loss = estimate_loss(model, val_loader, device, eval_iters)\n",
        "    return {'train': train_loss, 'val': val_loss}\n",
        "\n",
        "def print_gpu_memory_stats(checkpoint_name, device):\n",
        "    if torch.cuda.is_available() and device.type == 'cuda':\n",
        "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "        reserved = torch.cuda.memory_reserved(device) / (1024**2)\n",
        "        max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
        "        max_reserved = torch.cuda.max_memory_reserved(device) / (1024**2)\n",
        "\n",
        "        print(f\"--- GPU Memory Stats at: {checkpoint_name} ({device}) ---\")\n",
        "        print(f\"  Current Allocated: {allocated:.2f} MB\")\n",
        "        print(f\"  Current Reserved:  {reserved:.2f} MB\")\n",
        "        print(f\"  Peak Allocated:    {max_allocated:.2f} MB\")\n",
        "        print(f\"  Peak Reserved:     {max_reserved:.2f} MB\")\n",
        "        print(\"----------------------------------------------------\")\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, gen_f, cfg, logger):\n",
        "    device = torch.device(cfg.device) # Ensure device object\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        print(\"Starting training on CUDA device. Initializing memory stats.\")\n",
        "        # Reset peak stats at the beginning of training if you want to track peaks per training run\n",
        "        torch.cuda.reset_peak_memory_stats(device)\n",
        "        print_gpu_memory_stats(\"Start of training_model\", device)\n",
        "\n",
        "    print(f\"started training model with {cfg.n_params:_} parameters. model parameters file: {cfg.save_filename}\")\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        print(f\"[{epoch + 1} / {cfg.num_epochs:5d}]: starting at {datetime.now()}, will show running loss every {cfg.log_interval} steps, will eval every {cfg.eval_interval} steps\")\n",
        "        if device.type == 'cuda':\n",
        "            print_gpu_memory_stats(f\"Start of Epoch {epoch + 1}\", device)\n",
        "\n",
        "        for i, (X, Y) in enumerate(train_loader):\n",
        "            step += 1\n",
        "\n",
        "            X, Y = X.to(cfg.device), Y.to(cfg.device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            logits, loss = model(X, Y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if (i + 1) % cfg.eval_interval == 0:\n",
        "                losses = evaluate_model(model, train_loader, val_loader, device, eval_iters=cfg.eval_iters)\n",
        "                print(f\"[{epoch + 1} / {i + 1:5d}]: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}, eval_iters: {cfg.eval_iters}\")\n",
        "                completion = gen_f(model)\n",
        "                print(f\"[{epoch + 1} / {i + 1:5d}]: {completion}\")\n",
        "                print_gpu_memory_stats(f\"[{epoch + 1}  {i + 1:5d}]\", device)\n",
        "                torch.save(model._orig_mod.state_dict(), cfg.save_filename)\n",
        "                logger.log(\n",
        "                        train_loss=losses['train'],\n",
        "                        val_loss=losses['val'],\n",
        "                        running_loss=(running_loss / cfg.log_interval),\n",
        "                        step=step,\n",
        "                        epoch=epoch + 1,\n",
        "                        generated_text=wandb.Table(columns=[\"step\", \"text\"], rows=[[step, completion]])\n",
        "                    )\n",
        "                running_loss = 0\n",
        "            elif (i + 1) % cfg.log_interval == 0:\n",
        "                print(f\"[{epoch + 1} / {i + 1:5d}]: running loss {running_loss / cfg.log_interval:.3f}\")\n",
        "                logger.log(\n",
        "                        running_loss=(running_loss / cfg.log_interval),\n",
        "                        step=step\n",
        "                    )\n",
        "                running_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6",
      "metadata": {
        "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6"
      },
      "outputs": [],
      "source": [
        "from llm_e2e import WandbLogger\n",
        "\n",
        "model = GPT2Model(cfg)\n",
        "cfg.save_filename = \"/content/drive/MyDrive/llm_e2e/\" + cfg.save_filename\n",
        "print(f\"save_filename: {cfg.save_filename}\")\n",
        "\n",
        "load_weights = True\n",
        "if load_weights and os.path.exists(cfg.save_filename):\n",
        "    params = torch.load(cfg.save_filename, weights_only=True)\n",
        "    model.load_state_dict(params)\n",
        "    print(f\"loaded model weights: {cfg.save_filename}\")\n",
        "\n",
        "if cfg.device == 'cuda':\n",
        "    model.to(torch.bfloat16)\n",
        "\n",
        "model.to(cfg.device)\n",
        "\n",
        "if cfg.compile_model:\n",
        "    model = torch.compile(model)\n",
        "\n",
        "gen_f = lambda m: generate_text(m, encoding, \"Paris is\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
        "with WandbLogger(cfg, model) as logger:\n",
        "    train_model(model, train_dataset, val_dataset, optimizer, gen_f, cfg=cfg, logger=logger)\n",
        "\n",
        "# disconnect the colab runtime\n",
        "if in_google_colab():\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7623b090-c097-4b8d-a040-4704e6e213a0",
      "metadata": {
        "id": "7623b090-c097-4b8d-a040-4704e6e213a0"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def safe_delete(var_names: list[str]) -> None:\n",
        "    \"\"\"\n",
        "    attempts to delete specified variables from local and global scopes\n",
        "    to facilitate garbage collection by removing name bindings.\n",
        "    \"\"\"\n",
        "    for var_name in var_names:\n",
        "      try:\n",
        "          del locals()[var_name]\n",
        "      except:\n",
        "        try:\n",
        "            del globals()[var_name]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "safe_delete(['model', 'optimizer', 'train_loader', 'val_loader'])\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}