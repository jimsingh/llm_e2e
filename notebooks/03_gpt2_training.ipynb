{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad511bae-86d5-4b9d-a0ac-bed20db8e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m169 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m165 packages\u001b[0m \u001b[2min 0.22ms\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimsingh/src/llm_e2e/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=5, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... estimated batches: 58\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "Total tokens analyzed: 102,400\n",
      "Unique tokens: 7013\n",
      "Top 10 tokens:\n",
      "  ID 198   ('\\n'      ): 12,382 (0.1209)\n",
      "  ID 11    (','       ): 5,909  (0.0577)\n",
      "  ID 25    (':'       ): 3,139  (0.0307)\n",
      "  ID 13    ('.'       ): 2,362  (0.0231)\n",
      "  ID 262   (' the'    ): 1,753  (0.0171)\n",
      "  ID 284   (' to'     ): 1,298  (0.0127)\n",
      "  ID 286   (' of'     ): 1,090  (0.0106)\n",
      "  ID 290   (' and'    ): 1,083  (0.0106)\n",
      "  ID 26    (';'       ): 1,003  (0.0098)\n",
      "  ID 314   (' I'      ): 997    (0.0097)\n",
      "Vocabulary coverage: 0.13954275026364488\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "x: [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198]\n",
      "y: [22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198]\n",
      "x: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n'\n",
      "y: ' Citizen:\\nBefore we proceed any further, hear me speak.\\n\\n'\n",
      "x: [514, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546]\n",
      "y: [13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546, 408]\n",
      "x: ' us.\\n\\nMENENIUS:\\nEither you must\\nConf'\n",
      "y: '.\\n\\nMENENIUS:\\nEither you must\\nConfess'\n"
     ]
    }
   ],
   "source": [
    "%run -n 00_config.ipynb\n",
    "%run -n 01_data_pipeline.ipynb\n",
    "%run -n 02_gpt2_model.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e5f906-5761-44e9-85a4-009c8299c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt: str, max_tokens=20) -> str:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    encoded_ids = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        output_token_ids = model.generate(encoded_ids, max_tokens)\n",
    "    \n",
    "    decoded_ids_list = output_token_ids[0].cpu().tolist()\n",
    "    decoded_text = tokenizer.decode(decoded_ids_list)\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "def estimate_loss(model, loader, device, eval_iters):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for i, (X, Y) in enumerate(itertools.islice(loader, eval_iters)):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "@torch.no_grad\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: to evaluate\n",
    "      train_loader: training dataset iterator\n",
    "      val_loader: validation dataset iterator\n",
    "      eval_iters: the number of iterations to pull from the loaders\n",
    "\n",
    "    Returns:\n",
    "      dict with 'train' and 'val' loss\n",
    "  \"\"\"\n",
    "    train_loss = estimate_loss(model, train_loader, device, eval_iters)\n",
    "    val_loss = estimate_loss(model, val_loader, device, eval_iters)\n",
    "    return {'train': train_loss, 'val': val_loss}\n",
    "\n",
    "def print_gpu_memory_stats(checkpoint_name, device):\n",
    "    if torch.cuda.is_available() and device.type == 'cuda':\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        reserved = torch.cuda.memory_reserved(device) / (1024**2)\n",
    "        max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "        max_reserved = torch.cuda.max_memory_reserved(device) / (1024**2)\n",
    "\n",
    "        print(f\"--- GPU Memory Stats at: {checkpoint_name} ({device}) ---\")\n",
    "        print(f\"  Current Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"  Current Reserved:  {reserved:.2f} MB\")\n",
    "        print(f\"  Peak Allocated:    {max_allocated:.2f} MB\")\n",
    "        print(f\"  Peak Reserved:     {max_reserved:.2f} MB\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        \n",
    "def train_model(model, train_loader, val_loader, optimizer, gen_f, cfg):\n",
    "    device = torch.device(cfg.device) # Ensure device object\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        print(\"Starting training on CUDA device. Initializing memory stats.\")\n",
    "        # Reset peak stats at the beginning of training if you want to track peaks per training run\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        print_gpu_memory_stats(\"Start of training_model\", device)\n",
    "    \n",
    "    print(f\"started training model with {cfg.n_params:_} parameters. checkpoint file: {cfg.save_filename}\")\n",
    "    \n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"[{epoch + 1} / {cfg.num_epochs}]: starting at {datetime.now()}, will show running loss every {cfg.log_interval} steps, will eval every {cfg.eval_interval} steps\")\n",
    "        if device.type == 'cuda':\n",
    "            print_gpu_memory_stats(f\"Start of Epoch {epoch + 1}\", device)\n",
    "\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "            X, Y = X.to(cfg.device), Y.to(cfg.device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            logits, loss = model(X, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % cfg.log_interval == 0:\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: running loss {running_loss / cfg.log_interval:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if (i + 1) % cfg.eval_interval == 0:\n",
    "                losses = evaluate_model(model, train_loader, val_loader, device, eval_iters=cfg.eval_iters)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}, eval_iters: {cfg.eval_iters}\")\n",
    "                completion = gen_f(model)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: {completion}\")\n",
    "                print_gpu_memory_stats(f\"[{epoch + 1}  {i + 1:5d}]\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded config from: gpt2_config_wikipedia_cpu.yaml\n",
      "started training model with 22_939_776 parameters. checkpoint file: rahular_simple-wikipedia.22939776.pt\n",
      "[1 / 10]: starting at 2025-06-07 00:03:43.610988, will show running loss every 10 steps, will eval every 50 steps\n",
      "[1     10]: running loss 9.650\n",
      "[1     20]: running loss 7.908\n",
      "[1     30]: running loss 7.403\n"
     ]
    }
   ],
   "source": [
    "cfg = GPT2Config().from_yaml(\"gpt2_config_wikipedia_cpu.yaml\")\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "gen_f = lambda m: generate_text(m, enc, \"it was the happiest of times\")\n",
    "\n",
    "train_loader = GeneratorWrapper(cfg, enc)\n",
    "val_loader = GeneratorWrapper(cfg, enc)\n",
    "\n",
    "try:\n",
    "  del model\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  del optimizer\n",
    "except:\n",
    "  pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    assert cfg.device == 'cuda'\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    if capability[0] >= 7:  # Volta (7.0+), Turing (7.5+), Ampere (8.0+), Hopper (9.0+)\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        print(\"Uses tensor cores\")\n",
    "    else:\n",
    "        print(\"Tensor cores not supported on this GPU. Using default precision.\")\n",
    "    print(f\"Uses tensor cores: {torch.cuda.is_available()}\")\n",
    "    \n",
    "model = GPTModel(cfg)\n",
    "if cfg.device == 'cuda':\n",
    "    model.to(torch.bfloat16)\n",
    "\n",
    "# Load the cleaned state_dict into the new model\n",
    "if False:\n",
    "    model.load_state_dict(new_state_dict)\n",
    "model.to(cfg.device)\n",
    "if cfg.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "train_model(model, train_loader, val_loader, optimizer, gen_f, cfg=cfg)\n",
    "\n",
    "torch.save(model, cfg.save_filename, weights_only=False)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362a263-cefd-4f8f-9eb1-93b1a42e68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_path = 'gpt2_training_fineweb-edu_47000_steps.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "# Load the saved state_dict\n",
    "if False:\n",
    "    state_dict = torch.load('foo.pt')\n",
    "    \n",
    "    # Create a new state_dict without the \"_orig_mod.\" prefix\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_key = k[len(\"_orig_mod.\"):]  # Remove the prefix\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v # If for some reason some keys don't have it\n",
    "    \n",
    "    # Instantiate the model\n",
    "    cfg = GPT2Config().from_yaml(\"gpt2_config_gpu.yaml\")\n",
    "    enc = tiktoken.get_encoding('gpt2')\n",
    "    \n",
    "    # Ensure cfg here is compatible with the model architecture whose weights were saved\n",
    "    model = GPTModel(cfg) # Your actual model instantiation\n",
    "    \n",
    "    # Load the cleaned state_dict into the new model\n",
    "    model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623b090-c097-4b8d-a040-4704e6e213a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_gpu_memory_stats('foo', torch.device(cfg.device))\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "# Assuming 'my_lingering_tensor_var' is a variable holding a tensor\n",
    "# or 'my_list_of_tensors' is a list holding them.\n",
    "\n",
    "# If you know the variable names:\n",
    "# del my_lingering_tensor_var\n",
    "# del my_list_of_tensors\n",
    "\n",
    "# Then run garbage collection\n",
    "gc.collect()\n",
    "if False:\n",
    "  del model, optimizer, train_loader, val_loader\n",
    "\n",
    "gc.collect()\n",
    "# It's also good practice to clear PyTorch's cache AFTER Python references are gone\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
