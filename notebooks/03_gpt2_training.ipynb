{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982e568-5e5e-4868-a4c1-8e95b51f1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device) -> float:\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "import math\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        #print(f\"probas.shape: {probas.shape}, \" + str(probas[0, [0, 1, 2]]))\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        #print(f\"Token Ids:\\n {idx_next} -> {idx}\")\n",
    "    return idx\n",
    "\n",
    "\n",
    "END_OF_TEXT = '<|endoftext|>'\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={END_OF_TEXT})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # (T) -> (B, T)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # (B, T) -> (T)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device) -> float:\n",
    "    input_batch = input_batch.to(device, non_blocking=True)\n",
    "    target_batch = target_batch.to(device, non_blocking=True)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss(loader, model, device, num_batches=None) -> float:\n",
    "    i = 0\n",
    "    total_loss = 0\n",
    "    processed_batches = 0\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(loader):\n",
    "        if i >= num_batches: break\n",
    "\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += loss\n",
    "        processed_batches += 1\n",
    "\n",
    "    return total_loss / processed_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.position_embeddings.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model=model, idx=encoded, max_new_tokens=20, context_size=context_size)\n",
    "\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(\"decoded text: [\" + decoded_text +\"]\\n\")\n",
    "    model.train()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer,\n",
    "                start_epoch=0, initial_global_step=-1, initial_tokens_seen=0, initial_best_val_loss=math.inf,\n",
    "                initial_train_losses=None, initial_val_losses=None, initial_track_tokens_seen=None,\n",
    "                checkpoint_path=\"latest_checkpoint.pth\", best_model_path=\"best_model_params.pth\"):\n",
    "\n",
    "    # Initialize from loaded/default states\n",
    "    train_losses = initial_train_losses if initial_train_losses is not None else []\n",
    "    val_losses = initial_val_losses if initial_val_losses is not None else []\n",
    "    track_tokens_seen = initial_track_tokens_seen if initial_track_tokens_seen is not None else []\n",
    "    tokens_seen = initial_tokens_seen\n",
    "    global_step = initial_global_step\n",
    "    best_val_loss = initial_best_val_loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "            input_batch = input_batch.to(device, non_blocking=True)\n",
    "            target_batch = target_batch.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Common max_norm value\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            # xm.mark_step() # xla / tpu\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"epoch {epoch+1} step {global_step:06d}: train loss {train_loss:0.3f}, val loss: {val_loss:0.3f}\")\n",
    "                if val_loss < best_val_loss:\n",
    "                    torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "                    best_val_loss = val_loss\n",
    "                print_sample(model, tokenizer, device, start_context)\n",
    "                save_checkpoint(epoch, global_step, model, optimizer, tokens_seen, best_val_loss,\n",
    "                                train_losses, val_losses, track_tokens_seen, CHECKPOINT_PATH)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "import torch._dynamo\n",
    "import os\n",
    "\n",
    "#os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "#os.environ[\"TORCH_LOGS\"] = \"+dynamo,inductor\" # Get logs from both\n",
    "config['batch_size']=28\n",
    "print(f\"batch size: {config['batch_size']}\")\n",
    "\n",
    "\n",
    "print(\"Creating dataloaders ... \", end=\"\")\n",
    "train_loader = create_dataloader(train_dataset, tokenizer=enc,\n",
    "                                    batch_size=config['batch_size'],\n",
    "                                    max_length=config['context_length'],\n",
    "                                    stride=config['context_length'])\n",
    "\n",
    "val_loader = create_dataloader(val_dataset, tokenizer=enc,\n",
    "                                    batch_size=config['batch_size'],\n",
    "                                    max_length=config['context_length'],\n",
    "                                    stride=config['context_length'])\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "model = GPTModel(config)\n",
    "model.to(device)\n",
    "if device == 'cuda':\n",
    "  model.to(torch.bfloat16)\n",
    "\n",
    "model = torch.compile(model) #, backend='openxla')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "CHECKPOINT_PATH = '/content/drive/MyDrive/colab/llm_e2e/training_checkpoint.pkl'\n",
    "BEST_MODEL_PATH = '/content/drive/MyDrive/colab/llm_e2e/parameters.pth'\n",
    "#loaded_states = load_checkpoint(CHECKPOINT_PATH, model, optimizer, device)\n",
    "\n",
    "num_epochs=50\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters: {total_params:,}')\n",
    "\n",
    "# --- Start Training ---\n",
    "# Pass the loaded states to train_model\n",
    "train_losses_log, val_losses_log, tokens_seen_log = train_model(\n",
    "    model, overfit_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=1, eval_iter=5,\n",
    "    start_context=\"the fastest way to\", tokenizer=enc,\n",
    "    start_epoch=loaded_states['start_epoch'],\n",
    "    initial_global_step=loaded_states['global_step'],\n",
    "    initial_tokens_seen=loaded_states['tokens_seen'],\n",
    "    initial_best_val_loss=loaded_states['best_val_loss'],\n",
    "    initial_train_losses=loaded_states['train_losses'],\n",
    "    initial_val_losses=loaded_states['val_losses'],\n",
    "    initial_track_tokens_seen=loaded_states['track_tokens_seen'],\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    best_model_path=BEST_MODEL_PATH\n",
    ")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5f906-5761-44e9-85a4-009c8299c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.08ms\u001b[0m\u001b[0m\n",
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=5, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... estimated batches: 58\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "Total tokens analyzed: 102,400\n",
      "Unique tokens: 7013\n",
      "Top 10 tokens:\n",
      "  ID 198   ('\\n'      ): 12,382 (0.1209)\n",
      "  ID 11    (','       ): 5,909  (0.0577)\n",
      "  ID 25    (':'       ): 3,139  (0.0307)\n",
      "  ID 13    ('.'       ): 2,362  (0.0231)\n",
      "  ID 262   (' the'    ): 1,753  (0.0171)\n",
      "  ID 284   (' to'     ): 1,298  (0.0127)\n",
      "  ID 286   (' of'     ): 1,090  (0.0106)\n",
      "  ID 290   (' and'    ): 1,083  (0.0106)\n",
      "  ID 26    (';'       ): 1,003  (0.0098)\n",
      "  ID 314   (' I'      ): 997    (0.0097)\n",
      "Vocabulary coverage: 0.13954275026364488\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "x: [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198]\n",
      "y: [22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198]\n",
      "x: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n'\n",
      "y: ' Citizen:\\nBefore we proceed any further, hear me speak.\\n\\n'\n",
      "x: [514, 13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546]\n",
      "y: [13, 198, 198, 49275, 1677, 40, 2937, 25, 198, 32478, 345, 1276, 198, 18546, 408]\n",
      "x: ' us.\\n\\nMENENIUS:\\nEither you must\\nConf'\n",
      "y: '.\\n\\nMENENIUS:\\nEither you must\\nConfess'\n",
      "\u001b[2mResolved \u001b[1m145 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m140 packages\u001b[0m \u001b[2min 0.14ms\u001b[0m\u001b[0m\n",
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=24, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... estimated batches: 12\n",
      "ShakespeareDataloader Initializing: karpathy/tiny_shakespeare with B=24, T=1024, split='train'\n",
      "ShakespeareDataloader Pre-tokenizing text data n=1,003,854 for split 'train'... estimated batches: 12\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n",
      "ShakespeareDataloader iterator reset for split 'train', starting at token 0\n"
     ]
    }
   ],
   "source": [
    "%run -n 00_config.ipynb\n",
    "%run -n 01_data_pipeline.ipynb\n",
    "%run -n 02_gpt2_model.ipynb\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "@torch.no_grad\n",
    "def estimate_loss(model, loader, eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for i, (X, Y) in enumerate(itertools.islice(loader, eval_iters)):\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, eval_iters):\n",
    "    train_loss = calc_loss(model, train_loader, eval_iter)\n",
    "    val_loss = calc_loss(model, val_loader, eval_iter)\n",
    "    return train_loss, val_loss\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, optimizer, cfg):\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            train_loss = estimate_loss(model, train_loader, cfg.eval_interval)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += train_loss.item()\n",
    "            if i % cfg.eval_interval == 0:\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / cfg.eval_interval:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "cfg = GPT2Config.from_yaml(\"gpt2_config.yaml\")\n",
    "gpt2 = tiktoken.get_encoding('gpt2')\n",
    "tokenizer = lambda r: {'tokens': gpt2.encode_batch(r['text'], allowed_special={\"<|endoftext|>\"})} # endoftext may separate documents\n",
    "\n",
    "train_loader = ShakespeareDataloader(batch_size=cfg.batch_size, sequence_length=cfg.context_length, tokenizer=tokenizer)\n",
    "val_loader = ShakespeareDataloader(batch_size=cfg.batch_size, sequence_length=cfg.context_length, tokenizer=tokenizer, split='test')\n",
    "\n",
    "model = GPTModel(cfg)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "train_model(model, train_loader, val_loader, optimizer, cfg=cfg)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.n_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
