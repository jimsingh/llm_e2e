{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad511bae-86d5-4b9d-a0ac-bed20db8e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run -n 00_config.ipynb\n",
    "%run -n 01_data_pipeline.ipynb\n",
    "%run -n 02_gpt2_model.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5f906-5761-44e9-85a4-009c8299c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def estimate_loss(model, loader, device, eval_iters):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for i, (X, Y) in enumerate(itertools.islice(loader, eval_iters)):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "@torch.no_grad\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: to evaluate\n",
    "      train_loader: training dataset iterator\n",
    "      val_loader: validation dataset iterator\n",
    "      eval_iters: the number of iterations to pull from the loaders\n",
    "\n",
    "    Returns:\n",
    "      dict with 'train' and 'val' loss\n",
    "  \"\"\"\n",
    "    train_loss = estimate_loss(model, train_loader, device, eval_iters)\n",
    "    val_loss = estimate_loss(model, val_loader, device, eval_iters)\n",
    "    return {'train': train_loss, 'val': val_loss}\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, cfg):\n",
    "    device = torch.device(cfg.device) # Ensure device object\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        print(\"Starting training on CUDA device. Initializing memory stats.\")\n",
    "        # Reset peak stats at the beginning of training if you want to track peaks per training run\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        print_gpu_memory_stats(\"Start of training_model\", device)\n",
    "\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"[{epoch + 1} / {cfg.num_epochs}]: starting at {datetime.now()}, will log every {cfg.log_interval} steps\")\n",
    "        if device.type == 'cuda':\n",
    "            print_gpu_memory_stats(f\"Start of Epoch {epoch + 1}\", device)\n",
    "\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "            X, Y = X.to(cfg.device), Y.to(cfg.device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            logits, loss = model(X, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % cfg.log_iterval == 0:\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: running loss {running_loss / cfg.log_iterval:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if (i + 1) % cfg.eval_interval == 0:\n",
    "                losses = evaluate_model(model, train_loader, val_loader, device, eval_iters=cfg.eval_iters)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}, eval_iters: {cfg.eval_iters}\")\n",
    "                running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GPT2Config().from_yaml(\"gpt2_config.yaml\")\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "train_loader = GeneratorWrapper(cfg, enc)\n",
    "val_loader = GeneratorWrapper(cfg, enc)\n",
    "\n",
    "try:\n",
    "  del model\n",
    "except:\n",
    "  pass\n",
    "\n",
    "try:\n",
    "  del optimizer\n",
    "except:\n",
    "  pass\n",
    "\n",
    "model = GPTModel(cfg)\n",
    "model.to(cfg.device)\n",
    "if cfg.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "train_model(model, train_loader, val_loader, optimizer, cfg=cfg)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
