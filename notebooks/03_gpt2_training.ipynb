{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad511bae-86d5-4b9d-a0ac-bed20db8e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from llm_e2e import GPT2Config, GPT2Model, StreamingDatasetGenerator\n",
    "\n",
    "def setup_colab_env(project_root: str):\n",
    "    if 'google.colab' not in sys.modules:\n",
    "        return\n",
    "        \n",
    "    print(\"In Colab notebook. Installing dependencies ...\")\n",
    "    proj_path = Path(project_root)\n",
    "    if not (root_path / \"pyproject.toml\").is_file():\n",
    "        print(f\"Error: 'pyproject.toml' not found in {root_path}\")\n",
    "        return\n",
    "        \n",
    "    subprocess.run(\n",
    "        [\"uv\", \"sync\"],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    subprocess.run(\n",
    "        [\"uv\", \"pip\", \"install\", \"-e\", \".\"],\n",
    "        cwd=root_path,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "def setup_cuda(cfg: GPT2Config):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "        \n",
    "    assert cfg.device == 'cuda'\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    capability = torch.cTrueuda.get_device_capability()\n",
    "    if capability[0] >= 7:  # Volta (7.0+), Turing (7.5+), Ampere (8.0+), Hopper (9.0+)\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        print(\"Uses tensor cores\")\n",
    "    else:\n",
    "        print(\"Tensor cores not supported on this GPU.\")\n",
    "    \n",
    "project_root = '/home/jimsingh/src/llm_e2e/'\n",
    "setup_colab_env(project_root)\n",
    "config_yaml = f\"gpt2_bert_corpus_gpu.yaml\"\n",
    "cfg = GPT2Config.from_yaml(f\"{project_root}/config/{config_yaml}\")\n",
    "encoding = tiktoken.get_encoding(cfg.encoding_name)\n",
    "setup_cuda(cfg)\n",
    "\n",
    "train_dataset = StreamingDatasetGenerator(cfg, encoding=encoding)\n",
    "val_dataset = StreamingDatasetGenerator(cfg, encoding=encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5f906-5761-44e9-85a4-009c8299c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_text(model, tokenizer, prompt: str, max_tokens=20) -> str:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    encoded_ids = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        output_token_ids = model.generate(encoded_ids, max_tokens)\n",
    "    \n",
    "    decoded_ids_list = output_token_ids[0].cpu().tolist()\n",
    "    decoded_text = tokenizer.decode(decoded_ids_list)\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "def estimate_loss(model, loader, device, eval_iters):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for i, (X, Y) in enumerate(itertools.islice(loader, eval_iters)):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    model.train()\n",
    "    return losses.mean()\n",
    "\n",
    "@torch.no_grad\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      model: to evaluate\n",
    "      train_loader: training dataset iterator\n",
    "      val_loader: validation dataset iterator\n",
    "      eval_iters: the number of iterations to pull from the loaders\n",
    "\n",
    "    Returns:\n",
    "      dict with 'train' and 'val' loss\n",
    "  \"\"\"\n",
    "    train_loss = estimate_loss(model, train_loader, device, eval_iters)\n",
    "    val_loss = estimate_loss(model, val_loader, device, eval_iters)\n",
    "    return {'train': train_loss, 'val': val_loss}\n",
    "\n",
    "def print_gpu_memory_stats(checkpoint_name, device):\n",
    "    if torch.cuda.is_available() and device.type == 'cuda':\n",
    "        allocated = torch.cuda.memory_allocated(device) / (1024**2)\n",
    "        reserved = torch.cuda.memory_reserved(device) / (1024**2)\n",
    "        max_allocated = torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "        max_reserved = torch.cuda.max_memory_reserved(device) / (1024**2)\n",
    "\n",
    "        print(f\"--- GPU Memory Stats at: {checkpoint_name} ({device}) ---\")\n",
    "        print(f\"  Current Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"  Current Reserved:  {reserved:.2f} MB\")\n",
    "        print(f\"  Peak Allocated:    {max_allocated:.2f} MB\")\n",
    "        print(f\"  Peak Reserved:     {max_reserved:.2f} MB\")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        \n",
    "def train_model(model, train_loader, val_loader, optimizer, gen_f, cfg):\n",
    "    device = torch.device(cfg.device) # Ensure device object\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        print(\"Starting training on CUDA device. Initializing memory stats.\")\n",
    "        # Reset peak stats at the beginning of training if you want to track peaks per training run\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        print_gpu_memory_stats(\"Start of training_model\", device)\n",
    "    \n",
    "    print(f\"started training model with {cfg.n_params:_} parameters. model parameters file: {cfg.save_filename}\")\n",
    "    \n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print(f\"[{epoch + 1} / {cfg.num_epochs}]: starting at {datetime.now()}, will show running loss every {cfg.log_interval} steps, will eval every {cfg.eval_interval} steps\")\n",
    "        if device.type == 'cuda':\n",
    "            print_gpu_memory_stats(f\"Start of Epoch {epoch + 1}\", device)\n",
    "\n",
    "        for i, (X, Y) in enumerate(train_loader):\n",
    "            X, Y = X.to(cfg.device), Y.to(cfg.device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            logits, loss = model(X, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % cfg.log_interval == 0:\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: running loss {running_loss / cfg.log_interval:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "            if (i + 1) % cfg.eval_interval == 0:\n",
    "                losses = evaluate_model(model, train_loader, val_loader, device, eval_iters=cfg.eval_iters)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}, eval_iters: {cfg.eval_iters}\")\n",
    "                completion = gen_f(model)\n",
    "                print(f\"[{epoch + 1}  {i + 1:5d}]: {completion}\")\n",
    "                print_gpu_memory_stats(f\"[{epoch + 1}  {i + 1:5d}]\", device)\n",
    "                torch.save(model._orig_mod.state_dict(), cfg.save_filename) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefc52b-523d-41ac-be91-a47eaeab9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp\n",
    "load_full_model = True\n",
    "load_weights = False\n",
    "if load_full_model and os.path.exists(cfg.save_filename):\n",
    "    model = torch.load(cfg.save_filename, weights_only=False)\n",
    "    print(f\"loaded model weights: {cfg.save_filename}\")\n",
    "elif load_weights:\n",
    "    # Create a new state_dict without the \"_orig_mod.\" prefix\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_key = k[len(\"_orig_mod.\"):]  # Remove the prefix\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v # If for some reason some keys don't have it\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    model = GPTModel(cfg)\n",
    "\n",
    "\n",
    "\n",
    "if cfg.device == 'cuda':\n",
    "    model.to(torch.bfloat16)\n",
    "\n",
    "model.to(cfg.device)\n",
    "\n",
    "if cfg.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "gen_f = lambda m: generate_text(m, enc, \"Paris is\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "train_model(model, train_loader, val_loader, optimizer, gen_f, cfg=cfg)\n",
    "\n",
    "torch.save(model, cfg.save_filename)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362a263-cefd-4f8f-9eb1-93b1a42e68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model_path = 'gpt2_training_fineweb-edu_47000_steps.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "# Load the saved state_dict\n",
    "if False:\n",
    "    # Instantiate the model\n",
    "    cfg = GPT2Config().from_yaml(\"gpt2_config_wikipedia_cpu.yaml\")\n",
    "    enc = tiktoken.get_encoding(cfg.encoding_name)\n",
    "    state_dict = torch.load(cfg.save_filename)\n",
    "    \n",
    "    # Create a new state_dict without the \"_orig_mod.\" prefix\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_key = k[len(\"_orig_mod.\"):]  # Remove the prefix\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v # If for some reason some keys don't have it\n",
    "    \n",
    "    model = GPTModel(cfg)\n",
    "    model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623b090-c097-4b8d-a040-4704e6e213a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_gpu_memory_stats('foo', torch.device(cfg.device))\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "# Assuming 'my_lingering_tensor_var' is a variable holding a tensor\n",
    "# or 'my_list_of_tensors' is a list holding them.\n",
    "\n",
    "# If you know the variable names:\n",
    "# del my_lingering_tensor_var\n",
    "# del my_list_of_tensors\n",
    "\n",
    "# Then run garbage collection\n",
    "gc.collect()\n",
    "if False:\n",
    "  del model, optimizer, train_loader, val_loader\n",
    "\n",
    "gc.collect()\n",
    "# It's also good practice to clear PyTorch's cache AFTER Python references are gone\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
