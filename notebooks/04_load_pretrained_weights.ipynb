{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03faf1b3-22c0-4afc-8fdf-0662cae43a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# src: https://github.com/openai/gpt-2/blob/master/download_model.py\n",
    "def download_model(model):\n",
    "    subdir = os.path.join('../models/openai/', model)\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    subdir = subdir.replace('\\\\','/') \n",
    "    \n",
    "    for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\" + model + \"/\" + filename\n",
    "        print(url)\n",
    "        r = requests.get(url, stream=True)\n",
    "    \n",
    "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "\n",
    "weights = '1558M'\n",
    "download_model(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d34dadc-4bc0-4e92-833c-3059d216f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world, it's me who needs to make it through the next few rounds of training. It's me trying to learn the fundamentals of how to make\n"
     ]
    }
   ],
   "source": [
    "%run -n 00_config.ipynb\n",
    "%run -n 02_gpt2_model.ipynb\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_gpt2_checkpoint(checkpoint_path: str, model, config):\n",
    "    \"\"\"munge the parameter keys from the tf gpt2 checkpoint to match my model\"\"\"\n",
    "    \n",
    "    checkpoint = tf.train.load_checkpoint(checkpoint_path)\n",
    "    \n",
    "    pt_dict = {}\n",
    "\n",
    "    def transfer(pt_key, tf_key, transform=lambda x: x):\n",
    "        \"\"\" pt_dict[pt_key] <- tf_ckpt[tk_key] with an optional transform \"\"\"\n",
    "        tensor = torch.from_numpy(checkpoint.get_tensor(tf_key))\n",
    "        pt_dict[pt_key] = transform(tensor)\n",
    "    \n",
    "    def transfer_block(i, pt_suffix, tf_suffix, transform=lambda x: x):\n",
    "        transfer(f'transformer_blocks.{i}.{pt_suffix}', f'model/h{i}/{tf_suffix}', transform)\n",
    "    \n",
    "    # global weights (those outside of the transformer stack)\n",
    "    transfer('token_embedding.weight', 'model/wte')\n",
    "    transfer('position_embeddings.weight', 'model/wpe')\n",
    "    transfer('final_norm.gain', 'model/ln_f/g')\n",
    "    transfer('final_norm.bias', 'model/ln_f/b')\n",
    "    \n",
    "    # weight tying -- this works because we copy the reference\n",
    "    pt_dict['out_head.weight'] = pt_dict['token_embedding.weight']\n",
    "    \n",
    "    # layer weights\n",
    "    for i in range(config.n_layers):\n",
    "        # norms\n",
    "        transfer_block(i, 'norm1.gain', 'ln_1/g')\n",
    "        transfer_block(i, 'norm1.bias', 'ln_1/b')\n",
    "        transfer_block(i, 'norm2.gain', 'ln_2/g')\n",
    "        transfer_block(i, 'norm2.bias', 'ln_2/b')\n",
    "        \n",
    "        # GPT2 uses a combined weight vector for attention, I use split QKV weights\n",
    "        # because it's easier to understand (and I looked at Gemma2 1B which uses the\n",
    "        # same KV and only duplicates Q) \n",
    "        qkv_w = torch.from_numpy(checkpoint.get_tensor(f'model/h{i}/attn/c_attn/w')).squeeze(0)\n",
    "        q, k, v = torch.split(qkv_w, config.emb_dim, dim=1)\n",
    "        pt_dict[f'transformer_blocks.{i}.att.W_query.weight'] = q.T\n",
    "        pt_dict[f'transformer_blocks.{i}.att.W_key.weight'] = k.T\n",
    "        pt_dict[f'transformer_blocks.{i}.att.W_value.weight'] = v.T\n",
    "\n",
    "        # gpt2 uses qkv biases, so load them\n",
    "        assert config.qkv_bias, \"qkv biases are reqiured for openai gpt2 weights\"\n",
    "        if config.qkv_bias:\n",
    "            qkv_b = torch.from_numpy(checkpoint.get_tensor(f'model/h{i}/attn/c_attn/b'))\n",
    "            q_b, k_b, v_b = torch.split(qkv_b, config.emb_dim, dim=0)\n",
    "            pt_dict[f'transformer_blocks.{i}.att.W_query.bias'] = q_b\n",
    "            pt_dict[f'transformer_blocks.{i}.att.W_key.bias'] = k_b\n",
    "            pt_dict[f'transformer_blocks.{i}.att.W_value.bias'] = v_b\n",
    "        \n",
    "        transfer_block(i, 'att.out_proj.weight', 'attn/c_proj/w', lambda x: x.squeeze(0).T)\n",
    "        transfer_block(i, 'att.out_proj.bias', 'attn/c_proj/b')\n",
    "        \n",
    "        # multi-layer perceptron (feedforward exapnsion and projection)\n",
    "        transfer_block(i, 'ff.expansion.weight', 'mlp/c_fc/w', lambda x: x.squeeze(0).T)\n",
    "        transfer_block(i, 'ff.expansion.bias', 'mlp/c_fc/b')\n",
    "        transfer_block(i, 'ff.projection.weight', 'mlp/c_proj/w', lambda x: x.squeeze(0).T)\n",
    "\n",
    "        assert config.mlp_bias, \"mlp biases are required for openai gpt2 weights\"\n",
    "        if config.mlp_bias:\n",
    "            transfer_block(i, 'ff.projection.bias', 'mlp/c_proj/b')\n",
    "\n",
    "    # strict true is the default, but I really mean it\n",
    "    return model.load_state_dict(pt_dict, strict=True)\n",
    "\n",
    "def load_config_from_hparams(hparams_content: dict) -> GPT2Config:\n",
    "    \"\"\"\n",
    "    Loads GPT2Config from a dictionary formed from hparams.json parameters.\n",
    "    \"\"\"\n",
    "    hparams = hparams_content\n",
    "\n",
    "    config_params = {\n",
    "        \"vocab_size\": hparams[\"n_vocab\"],\n",
    "        \"context_length\": hparams[\"n_ctx\"],\n",
    "        \"block_size\": hparams[\"n_ctx\"],\n",
    "        \"emb_dim\": hparams[\"n_embd\"],\n",
    "        \"n_heads\": hparams[\"n_head\"],\n",
    "        \"n_layers\": hparams[\"n_layer\"],\n",
    "        \"qkv_bias\": True,     # default for OpenAI GPT-2 models\n",
    "        \"mlp_bias\": True,     # default for OpenAI GPT-2 models\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"compile_model\": True if torch.cuda.is_available() else False,\n",
    "        \"dtype\": \"bfloat16\" if torch.cuda.is_available() else \"float32\"\n",
    "    }\n",
    "\n",
    "    # instantiate GPT2Config with the mapped parameters\n",
    "    return GPT2Config(**config_params)\n",
    "    \n",
    "weights = '124M'\n",
    "model_path = f\"../models/openai/{weights}\"\n",
    "\n",
    "# tie it all together\n",
    "if 'm' not in locals() or reload_model:\n",
    "    print(f\"loading model from {model_path}\")\n",
    "    with open(f\"{model_path}/hparams.json\", 'r') as f:\n",
    "        hparams = json.load(f)\n",
    "        cfg = load_config_from_hparams(hparams)\n",
    "        enc = tiktoken.get_encoding(cfg.encoding_name)\n",
    "        m = GPTModel(cfg)\n",
    "        load_gpt2_checkpoint(model_path, m, cfg)\n",
    "        reload_model = False\n",
    "\n",
    "def gen_text(model, tokenizer, prompt: str, max_tokens=25) -> str:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    encoded_ids = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "\n",
    "    # Model inferenceb\n",
    "    with torch.no_grad():\n",
    "        output_token_ids = model.generate(encoded_ids, max_tokens, top_k=10) #, temperature=1.1, do_sample=True, top_k=None, top_p=0.995)\n",
    "    \n",
    "    decoded_ids_list = output_token_ids[0].cpu().tolist()\n",
    "    decoded_text = tokenizer.decode(decoded_ids_list)\n",
    "    return decoded_text\n",
    "    \n",
    "s = gen_text(m, enc, \"hello world, it's me\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b2bc4c9-af44-467d-acc0-947c2cbba87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world, it's me, the world's most boring person.\n",
      "\n",
      "I'm not sure if I'm the most interesting\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "\n",
    "# compare to hugging face transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-xl\")\n",
    "\n",
    "prompt = \"hello world, it's me\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=20,      # same number of tokens\n",
    "    num_beams=1,            # my generate doesn't have beam search\n",
    "    no_repeat_ngram_size=2, # Prevent repeating 2-grams\n",
    "    do_sample=False,         # Enable sampling\n",
    "    temperature=1.0,\n",
    "    top_p=0.95              # nucleous sampling vs top-k\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
