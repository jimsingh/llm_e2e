{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03faf1b3-22c0-4afc-8fdf-0662cae43a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# src: https://github.com/openai/gpt-2/blob/master/download_model.py\n",
    "def download_model(model):\n",
    "    subdir = os.path.join('../models/openai/', model)\n",
    "    if not os.path.exists(subdir):\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "    subdir = subdir.replace('\\\\','/') \n",
    "    \n",
    "    for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\" + model + \"/\" + filename\n",
    "        print(url)\n",
    "        r = requests.get(url, stream=True)\n",
    "    \n",
    "        with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)\n",
    "\n",
    "# download_model('124M')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d34dadc-4bc0-4e92-833c-3059d216f90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat in the pic from these lines) is pretty toothy as awry luck would dictate â€” Pastry Guide (@'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_gpt2_checkpoint(checkpoint_path: str, model, config):\n",
    "    \"\"\"Load TF GPT-2 checkpoint into PyTorch model.\"\"\"\n",
    "    checkpoint = tf.train.load_checkpoint(checkpoint_path)\n",
    "    pt_dict = {}\n",
    "    \n",
    "    def transfer(pt_key, tf_key, transform=None):\n",
    "        tensor = torch.from_numpy(checkpoint.get_tensor(tf_key))\n",
    "        pt_dict[pt_key] = transform(tensor) if transform else tensor\n",
    "    \n",
    "    def transfer_block(i, pt_suffix, tf_suffix, transform=None):\n",
    "        transfer(f'transformer_blocks.{i}.{pt_suffix}', f'model/h{i}/{tf_suffix}', transform)\n",
    "    \n",
    "    # Global weights (outside of transformer stack)\n",
    "    transfer('token_embedding.weight', 'model/wte')\n",
    "    transfer('position_embeddings.weight', 'model/wpe')\n",
    "    transfer('final_norm.gain', 'model/ln_f/g')\n",
    "    transfer('final_norm.bias', 'model/ln_f/b')\n",
    "    \n",
    "    # weight tying\n",
    "    pt_dict['out_head.weight'] = pt_dict['token_embedding.weight']\n",
    "    \n",
    "    # Per-layer weights\n",
    "    for i in range(config.n_layers):\n",
    "        # Norms\n",
    "        transfer_block(i, 'norm1.gain', 'ln_1/g')\n",
    "        transfer_block(i, 'norm1.bias', 'ln_1/b')\n",
    "        transfer_block(i, 'norm2.gain', 'ln_2/g')\n",
    "        transfer_block(i, 'norm2.bias', 'ln_2/b')\n",
    "        \n",
    "        # Attention - split QKV weights\n",
    "        qkv_w = torch.from_numpy(checkpoint.get_tensor(f'model/h{i}/attn/c_attn/w')).squeeze(0)\n",
    "        q, k, v = torch.split(qkv_w, config.emb_dim, dim=1)\n",
    "        pt_dict[f'transformer_blocks.{i}.att.W_query.weight'] = q.T\n",
    "        pt_dict[f'transformer_blocks.{i}.att.W_key.weight'] = k.T\n",
    "        pt_dict[f'transformer_blocks.{i}.att.W_value.weight'] = v.T\n",
    "        \n",
    "        if config.qkv_bias:\n",
    "            qkv_b = torch.from_numpy(checkpoint.get_tensor(f'model/h{i}/attn/c_attn/b'))\n",
    "            q_b, k_b, v_b = torch.split(qkv_b, config.emb_dim, dim=0)\n",
    "            pt_dict[f'transformer_blocks.{i}.att.W_query.bias'] = q_b\n",
    "            pt_dict[f'transformer_blocks.{i}.att.W_key.bias'] = k_b\n",
    "            pt_dict[f'transformer_blocks.{i}.att.W_value.bias'] = v_b\n",
    "        \n",
    "        transfer_block(i, 'att.out_proj.weight', 'attn/c_proj/w', lambda x: x.squeeze(0).T)\n",
    "        transfer_block(i, 'att.out_proj.bias', 'attn/c_proj/b')\n",
    "        \n",
    "        # MLP\n",
    "        transfer_block(i, 'ff.expansion.weight', 'mlp/c_fc/w', lambda x: x.squeeze(0).T)\n",
    "        transfer_block(i, 'ff.expansion.bias', 'mlp/c_fc/b')\n",
    "        transfer_block(i, 'ff.projection.weight', 'mlp/c_proj/w', lambda x: x.squeeze(0).T)\n",
    "        \n",
    "        if config.mlp_bias:\n",
    "            transfer_block(i, 'ff.projection.bias', 'mlp/c_proj/b')\n",
    "    \n",
    "    return model.load_state_dict(pt_dict, strict=False)\n",
    "\n",
    "\n",
    "\n",
    "#%run -n 00_config.ipynb\n",
    "#%run -n 02_gpt2_model.ipynb\n",
    "\n",
    "cfg = GPT2Config(qkv_bias=True, device='cpu')\n",
    "m = GPTModel(cfg)\n",
    "gpt = load_gpt2_checkpoint('/home/jimsingh/src/llm_e2e/models/openai/124M/', m, cfg)\n",
    "\n",
    "generate_text(m, enc, \"the cat in the\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
