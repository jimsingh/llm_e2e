data:
    dataset_path: 'HuggingFaceFW/fineweb-edu'
    dataset_name: 'sample-10BT'
    block_size: 1024

model:
    vocab_size: 50304 # gpt2 vocab word aligneed
    context_length: 512 
    emb_dim: 768 
    n_heads: 8
    n_layers: 12 
    dropout_rate: 0.00 # drop out emb don't even initialize
    qkv_bias: False
    mlp_bias: True

training:
    num_epochs: 1
    learning_rate: 0.001
    batch_size: 128
    log_interval: 300
    eval_iters: 30
    eval_interval: 5000
    save_interval: 5000
    warmup_steps: 10_000
    max_steps: 300_000

logging:
  wandb_log: True

system:
    device: cuda 
