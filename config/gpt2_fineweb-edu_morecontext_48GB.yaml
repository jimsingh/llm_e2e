data:
    dataset_path: 'HuggingFaceFW/fineweb-edu'
    dataset_name: 'sample-10BT'
    block_size: 1024

model:
    vocab_size: 50304 # gpt2 vocab word aligneed
    context_length: 512 
    emb_dim: 512
    n_heads: 8
    n_layers: 12 
    dropout_rate: 0.0 # pytorch will optimize dropout layers
    qkv_bias: False
    mlp_bias: True

training:
    num_epochs: 1
    learning_rate: 0.003
    batch_size: 160
    log_interval: 300
    eval_iters: 30
    eval_interval: 5000
    save_interval: 5000
    warmup_steps: 10_000
    max_steps: 300_000

logging:
  wandb_log: True

system:
    device: cuda 
