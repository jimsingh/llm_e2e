data:
    dataset_path: 'HuggingFaceFW/fineweb-edu'
    dataset_name: 'sample-10BT'
    block_size: 1024

model:
    vocab_size: 50304 # gpt2 vocab word aligneed
    context_length: 384 
    emb_dim: 384
    n_heads: 8
    n_layers: 12 
    dropout_rate: 0.01
    qkv_bias: False
    mlp_bias: True

training:
    num_epochs: 1
    learning_rate: 0.0006 # GPT-3
    batch_size: 192
    log_interval: 300
    eval_iters: 30
    eval_interval: 1500
    save_interval: 1500
    warmup_steps: 10_000
    max_steps: 300_000

logging:
  wandb_log: True

system:
    device: cuda 
