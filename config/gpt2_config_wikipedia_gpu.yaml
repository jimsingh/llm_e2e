data:
    dataset_path: 'wikimedia/wikipedia'
    dataset_name: '20231101.simple'
    block_size: 256

model:
    vocab_size: 50257 # gpt2 vocab
    context_length: 256
    emb_dim: 384
    n_heads: 4
    n_layers: 4
    dropout_rate: 0.1
    qkv_bias: False
    mlp_bias: True

training:
    num_epochs: 20
    learning_rate: 0.0003
    batch_size: 64
    log_interval: 10
    eval_iters: 10
    eval_interval: 50
    save_interval: 200

system:
    device: cuda 
